{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f339ad48-c1bf-4e5c-83c0-e73d2fc7a296",
   "metadata": {},
   "source": [
    "# Transformers Fine-tuning Step\n",
    "Fine-tuning step in the DistilBERT model pre-trained on our data, to later save the model and its respective trained weights.\n",
    "\n",
    "In this step we perform:\n",
    "* Model definition;\n",
    "* Fine-tuning the model;\n",
    "* Model evaluation;\n",
    "* Saving the model and trained weights.\n",
    "\n",
    "> **Note**: **Article on Medium** about the `fine-tuning step in the pre-trained transformers` of this system in Portuguese: [Sentiment Analysis About Marvel Comics (Part 3)â€Š-â€ŠDistilBERT Fine-tuning](https://medium.com/@guineves.py/2648e14c9123)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6776b-a00c-4755-8110-bfdf4188cc94",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Packages](#1)\n",
    "* [Loading the Data](#2)\n",
    "* [Transformers](#3)\n",
    "* [Transfer Learning](#4)\n",
    "    * [Fine-tuning](#4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf3166-e70e-4490-a37b-1c681cf8966d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Packages\n",
    "Packages that were used in the system:\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): provides APIs and tools to easily download and train state-of-the-art pretrained models;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks;\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): open source machine learning library;\n",
    "* [src](../src/): package with all the codes for all utility functions created for this system. Located inside the `../../src/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a4f5ea5-7a61-4b66-9a59-c6b7ab71e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Getting Obtaining the absolute normalized version of the project root path\n",
    "    os.path.join( # Concatenating the paths\n",
    "        os.getcwd(), # Getting the path of the notebooks directory\n",
    "        os.pardir, # Gettin the constant string used by the OS to refer to the parent directory\n",
    "        os.pardir\n",
    "    )\n",
    ")\n",
    "# Adding path to the list of strings that specify the search path for modules\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.transformers_finetuning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acb6b9-6ba5-49e1-a00d-b90a63beeb67",
   "metadata": {},
   "source": [
    "> **Note**: the codes for the utility functions used in this system are in the `transformers_finetuning.py` script within the `../../src/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead2267-48de-4889-aac5-dea58282b080",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Loading the Data\n",
    "Let's read each of the subsets within their respective directories within the `../../data/preprocessed/` directory and plot each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e38bf67-f145-47b7-bfa7-5149743df283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Validation set shape: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Test set shape: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset.load_from_disk('../../data/preprocessed/train_dataset')\n",
    "val_set = Dataset.load_from_disk('../../data/preprocessed/validation_dataset')\n",
    "test_set = Dataset.load_from_disk('../../data/preprocessed/test_dataset')\n",
    "print(f'Train set shape: {train_set}\\nValidation set shape: {val_set}\\nTest set shape: {test_set}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37646c-d5a8-4829-a7d7-76080f57e6d3",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## Transformers\n",
    "<img align='center' src='../../figures/transformers.png' style='width:400px;'>\n",
    "Transformer is a purely attention-based model that was developed by Google to solve some problems with RNNs, it is difficult to fully exploit the advantages of parallel computing, due to its problems arising from its sequential structure. In a seq2seq RNN, we need to go through each word of our input, sequentially by the encoder, and it is done in a similar sequential way by the decoder, without parallel computation. For this reason, there is not much room for parallel calculations. The more words we have in the input sequence, the more time it will take to process it.\n",
    "\n",
    "With large sequences, that is, with many $T$ sequential steps, information tends to get lost in the network (loss of information) and problems of vanishing gradients arise related to the length of our input sequences. LSTMs and GRUs help a little with these problems, but even these architectures stop working well when they try to process very long sequences due to the `information bottleneck`.\n",
    "* `Loss of information`: it is more difficult to know whether the subject is singular or plural as we move away from the subject.\n",
    "* `Vanishing gradients`: When we calculate backprop, the gradients can become very small and as a result the model will not learn anything.\n",
    "\n",
    "Transformers are based on attention and do not require any sequential calculation per layer, requiring only a single step. Furthermore, the gradient steps that need to be taken from the last output to the first output in a transformer are just 1. For RNNs, the number of steps increases with longer sequences. Finally, transformers do not suffer from problems of vanishing gradients related to the length of the sequences.\n",
    "\n",
    "Transformers do not use RNNs, attention is all we need, and only a few linear and non-linear transformations are usually included. The transformers model was introduced in 2017 by Google researchers, and since then, the transformer architecture has become standard for LLMs. Transformers have revolutionized the field of NLP.\n",
    "\n",
    "The transformer model uses `scaled dot-product attention`. The first form of attention is very efficient in terms of computation and memory, because it consists only of matrix multiplication operations. This engine is the core of the model and allows the transfomer to grow and become more complex, while being faster and using less memory than other comparable model architectures.\n",
    "\n",
    "In the transformer model, we will use the `Multi-Head Attention layer`, this layer is executed in parallel and has several $h$ scaled dot-product attention mechanisms and several linear transformations of input queries, keys and values. In this layer, linear transformations are trainable parameters.\n",
    "$$\\text{ Attention}(Q, K, V) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V$$\n",
    "<img align='center' src='../../figures/attention.png' style='width:600px;'>\n",
    "\n",
    "`Encoders` transformers start with a multi-head attention module that performs `self-attention` on the input sequence. This is followed by a residual connection and normalization, then a feed forward layer and another residual connection and normalization. The encoder layer is repeated $N_x$ times.\n",
    "* Self-attention: each word in the input corresponds to each other word in the input sequence\n",
    "* Thanks to the self-attention layer, the encoder will provide a contextual representation of each of our inputs.\n",
    "\n",
    "\n",
    "The `decoder` is built in a similar way to the encoder, with multi-head attention, residual connections and normalization modules. The first attention module is masked (`Masked Self-Attention`) so that each position only serves the previous positions, it blocks the flow of information to the left. The second attention module (`Encoder-Decoder Attention`) takes the encoder output and allows the decoder to attend to all items. This entire decoder layer is also repeated several $N_x$ times, one after the other.\n",
    "$$\\text{ Masked Self-Attention } = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + M \\right) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + \\text{ mask matrix } \\begin{pmatrix} 0 & -\\infty & -\\infty \\\\ 0 & 0 & -\\infty \\\\ 0 & 0 & 0 \\end{pmatrix} \\right)$$\n",
    "\n",
    "Transformers also incorporate a `positional encoding stage` ($PE$), which encodes the position of each input in the sequence, that is, the sequential information. This is necessary because transformers don't use RNNs, but word order is relevant to any language. Positional encoding is trainable, just like word embeddings.\n",
    "$$\\begin{align*}\n",
    "& \\text{PE}_{(\\text{pos, }2i)} = \\text{sin}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right) \\\\\n",
    "& \\text{PE}_{(\\text{pos, }2i + 1)} = \\text{cos}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "First, the embedding on the input is calculated and the positional encodings are applied. So, this goes to the encoder which consists of several layers of Multi-Head Attention modules, then the decoder receives the output sequence shifted one step to the right and the encoder outputs. The decoder output is transformed into output probabilities using a linear layer with a softmax activation. This architecture is easy to parallelize compared to RNNs models and can be trained much more efficiently on multiple GPUs. It can also be scaled to learn multiple tasks on increasingly larger datasets. Transformers are a great alternative to RNNs and help overcome these problems in NLP and many fields that process sequential data.\n",
    "\n",
    "We will fine-tune the DistilBERT model, which is a small, fast, cheap and lightweight Transformer model, trained by distilling the base BERT model. It has 40% fewer parameters than bert-base-uncased, runs 60% faster, and preserves more than 95% of BERT's performance as measured in the GLUE (General Language Understanding Evaluation) benchmark.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (ðŸ¤—) is the best resource for pre-trained transformers. Its open source libraries make it simple to download, fine-tune, and use transformer models like DeepSeek, BERT, Llama, T5, Qwen, GPT-2, and more. And the best part, we can use them together with TensorFlow, PyTorch or Flax. In this notebook, I use ðŸ¤— transformers to use the `DistilBERT` model for sentiment classification. For the pre-processing step, we use the tokenizer (in the notebook `03_preprocessing.ipynb`), and the DistilBERT checkpoint fine-tuning `distilbert-base-uncased-finetuned-sst-2-english` pre-trained in the code below. To do this, we initialize the DistilBertForSequenceClassification class and define the desired pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a315b-c133-4fac-a7f8-f175550ad4e2",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## Transfer Learning\n",
    "<img align='center' src='../../figures/transfer_learning.png' style='width:600px;'>\n",
    "\n",
    "One of the most powerful ideas in deep learning is that sometimes we can take the knowledge that the neural network learned in one task and apply that knowledge to a separate task. There are 3 main advantages to transfer learning:\n",
    "* Reduces training time.\n",
    "* Improves predictions.\n",
    "* Allows us to use smaller datasets.\n",
    "\n",
    "If we are creating a model, rather than training the weights from 0, from a random initialization, we often make much faster progress by downloading weights that someone else has already trained for days/weeks/months on the neural network architecture, use them as pre-training, and transfer them to a new, similar task that we might be interested in. This means that we can often download weights from open-source algorithms that other people took weeks or months to figure out, and then we use that as a really good initialization for our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9148a-39fe-4150-bc54-7976a67f4e45",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### Fine-tuning\n",
    "We take the weights from an existing pre-trained model using transfer learning and then tweak them a bit to ensure they work for the specific task we are working on. Let's say we pre-trained a model that predicts movie evaluation, and now we're going to create a model to evaluate courses. One way to do this is, by locking all the weights that we already have pre-trained, and then we add a new output layer, or perhaps, a new feed forward layer and an output layer that will be trained, while we keep the rest locked and then we only train our new network, the new layers that we just added. We can slowly unfreeze the layers, one at a time.\n",
    "\n",
    "Many of the low-level features that the pre-trained model learned from a very large corpus, like the structure of the text, the nature of the text, this can help our algorithm do better in the sentiment classification task and faster or with less data, because maybe the model has learned enough what the structures of different texts are like and some of that knowledge will be useful. After deleting the output layer of a pre-trained model, we don't necessarily need to create just the output layer, but we can create several new layers.\n",
    "\n",
    "We need to remove the output layer from the pre-trained model and add ours, because the neural network can have a softmax output layer that generates one of 1000 possible labels. So we remove this output layer and create our own output layer, in this case a sigmoid activation.\n",
    "\n",
    "* With a small training set, we think of the rest of the layers as `frozen`, so we freeze the parameters of these layers, and only train the parameters associated with our output layer. This way we will obtain very good performance, even with a small training set.\n",
    "\n",
    "* With a larger training set, we can freeze fewer layers and then train the layers that were not frozen and our new output layer. We can use the layers that are not frozen as initialization and use gradient descent from them, or we can also eliminate these layers that are not frozen and use our own new hidden layers and our own output layer. Any of these methods could be worth trying.\n",
    "\n",
    "* With a much larger training set, we use this pre-trained neural network and its weights as initialization and train the entire neural network, just changing the output layer, with labels that we care about.\n",
    "\n",
    "Setting the checkpoint of the pre-trained model that we will do the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b6dd67-2714-4583-81d0-f8db9e01fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e5df1-9a50-43d1-b08b-da5943921f30",
   "metadata": {},
   "source": [
    "Setting the hyperparameters, using the Hugging Face Trainer object to fine-tune the model.\n",
    "* The pre-trained model with fine-tuning is already being saved in the `../../models/` directory, defined when defining the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1585ffdb-779b-420d-81f3-65fbe73251af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2540' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2540/2540 7:12:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.592717</td>\n",
       "      <td>0.684184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.697679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.585440</td>\n",
       "      <td>0.712016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.488176</td>\n",
       "      <td>0.756529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.502221</td>\n",
       "      <td>0.761378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.474515</td>\n",
       "      <td>0.775508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.474097</td>\n",
       "      <td>0.764560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.448737</td>\n",
       "      <td>0.778758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.487454</td>\n",
       "      <td>0.767549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.445698</td>\n",
       "      <td>0.818367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.394227</td>\n",
       "      <td>0.821744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.485700</td>\n",
       "      <td>0.402096</td>\n",
       "      <td>0.823441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.470619</td>\n",
       "      <td>0.817294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.383477</td>\n",
       "      <td>0.831915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.392066</td>\n",
       "      <td>0.833519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.384125</td>\n",
       "      <td>0.846788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.389733</td>\n",
       "      <td>0.843293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.395148</td>\n",
       "      <td>0.846830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.388265</td>\n",
       "      <td>0.837942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.351318</td>\n",
       "      <td>0.849480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.309669</td>\n",
       "      <td>0.867014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.387884</td>\n",
       "      <td>0.869130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.325386</td>\n",
       "      <td>0.870885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.344718</td>\n",
       "      <td>0.875784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2540, training_loss=0.4370420598608302, metrics={'train_runtime': 25970.5016, 'train_samples_per_second': 0.782, 'train_steps_per_second': 0.098, 'total_flos': 2690677801500672.0, 'train_loss': 0.4370420598608302, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuning hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../../models/transformers_results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=20,\n",
    "    weight_decay=1e-1,\n",
    "    eval_strategy='steps',\n",
    "    lr_scheduler_type='reduce_lr_on_plateau',\n",
    "    logging_steps=100\n",
    ")\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=f1_metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff61dd2-b950-461e-b805-d43abf11a9ca",
   "metadata": {},
   "source": [
    "Evaluating model performance with fine-tuning in the train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38acb7b3-8745-410e-8db4-7f05352bb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluate: 0.9178\n",
      "Validation set evaluation: 0.8523\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set evaluate: {trainer.evaluate(train_set)[\"eval_f1_score\"]:.4f}\\nValidation set evaluation: {trainer.evaluate(val_set)[\"eval_f1_score\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bfb82-0c3e-430d-b4e4-5debe24f6ca5",
   "metadata": {},
   "source": [
    "Evaluating the performance of the final model with fine-tuning in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14f3b133-8ef9-46c0-9a67-517822821220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluate: 0.8518\n"
     ]
    }
   ],
   "source": [
    "print(f'Test set evaluate: {trainer.evaluate(test_set)[\"eval_f1_score\"]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
