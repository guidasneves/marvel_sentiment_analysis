{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debe9ec0-89c7-46ab-9203-b210db8004cc",
   "metadata": {},
   "source": [
    "# Preprocessing Step\n",
    "Data pre-processing stage and then we feed this data to the model.\n",
    "\n",
    "In this step we perform:\n",
    "* Transformation of all words to lowercase;\n",
    "* Removal of punctuations;\n",
    "* Removal of stopwords;\n",
    "* We apply stemming;\n",
    "* Splitting between training, validation and testing;\n",
    "* Tokenization;\n",
    "* Padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa711489-0961-4df2-af39-63583899af35",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Packages](#1)\n",
    "* [Preprocessing](#2)\n",
    "    * [RNN Preprocessing](#2.1)\n",
    "        * [Lowercasing, Stopwords, Stemming and Punctuations](#2.1.1)\n",
    "        * [Tokenization](#2.1.2)\n",
    "    * [Transformer Preprocessing](#2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec45f-ccfd-46c9-913b-601cf152c125",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Packages\n",
    "Packages that were used in the system:\n",
    "* [pandas](https://pandas.pydata.org/): is the main package for data manipulation;\n",
    "* [numpy](www.numpy.org): is the main package for scientific computing;\n",
    "* [re](https://docs.python.org/3/library/re.html): provides regular expression matching operations similar to those found in Perl;\n",
    "* [string](https://docs.python.org/pt-br/3.13/library/string.html): for common string operations;\n",
    "* [nltk](https://www.nltk.org/): NLTK is a leading platform for building Python programs to work with human language data;\n",
    "* [tensorflow](https://www.tensorflow.org/): framework that makes it easy to create ML models that can run in any environment;\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): open source machine learning library;\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html): implements binary protocols for serializing and de-serializing a Python object structure;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): provides APIs and tools to easily download and train state-of-the-art pretrained models;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks;\n",
    "* [os](https://docs.python.org/3/library/os.html): built-in module, provides a portable way of using operating system dependent functionality;\n",
    "* [sys](https://docs.python.org/3/library/sys.html): provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter;\n",
    "* [src](../src/): package with all the codes for all utility functions created for this system. Located inside the `../../src/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375d1727-909a-4086-ba17-f377c05c39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Getting Obtaining the absolute normalized version of the project root path\n",
    "    os.path.join( # Concatenating the paths\n",
    "        os.getcwd(), # Getting the path of the notebooks directory\n",
    "        os.pardir, # Gettin the constant string used by the OS to refer to the parent directory\n",
    "        os.pardir\n",
    "    )\n",
    ")\n",
    "# Adding path to the list of strings that specify the search path for modules\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb33b25-3901-447b-9a3d-3aaaf12f60ad",
   "metadata": {},
   "source": [
    "> **Note**: the codes for the utility functions used in this system are in the `preprocessing.py` script within the `../../src/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e78a5-0cc8-4cd6-b8f2-4373e4035379",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Preprocessing\n",
    "We will create 2 models for this project. Therefore, we will do 2 different preprocessings, one for each model:\n",
    "1. The first will be using an RNN with a bidirectional LSTM layer created and trained from scratch.\n",
    "2. The second model will be the application of fine-tuning to a model with pre-trained transformers architecture.\n",
    "\n",
    "Reading the dataset that will be pre-processed and plotting your first 5 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e195ab81-50b4-4e6a-8eb9-0baaa1431269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORPâ€™s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP?s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...  non-action  \n",
       "1  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "2  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "3  A SHARK IN THE WATER! After X-CORPâ€™s shocking ...  non-action  \n",
       "4  A SHARK IN THE WATER! After X-CORP?s shocking ...  non-action  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data = pd.read_csv('../../data/raw/comics_corpus.csv')\n",
    "comics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a77c2-c521-447b-ad4a-3db973b379b6",
   "metadata": {},
   "source": [
    "We can see some duplicate phrases in the `description` feature. But if we explore more closely, we can see that what doesn't make them 100% similar are the scores. Therefore, let's treat the scores to eliminate duplicate examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f25a5f4-ec2e-4758-b71c-95fd9107f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some example:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isnâ€™t Helâ€™s only rulerâ€”and now sheâ€™s upset the cosmic balance. There will be a price to payâ€¦and Karnilla intends to ensure the Valkyries pay it.\n",
      "\n",
      "Duplicated example:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn?t Hel?s only ruler?and now she?s upset the cosmic balance. There will be a price to pay?and Karnilla intends to ensure the Valkyries pay it.\n"
     ]
    }
   ],
   "source": [
    "print(f'Some example:\\n{comics_data[\"description\"][1]}\\n')\n",
    "print(f'Duplicated example:\\n{comics_data[\"description\"][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957d92e-6fad-41e5-b368-00a1dca68cbb",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### RNN Preprocessing\n",
    "For a sentiment classifier, we first pre-process the raw data, then tokenize our train set and extract useful features to train our model and make our predictions.\n",
    "\n",
    "Plotting the stopwords in English and the punctuations that will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61015b4e-cfc4-4c92-b2cb-0675e5125239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuations:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "print(f'Stopwords:\\n{stopwords_en}\\n\\nPunctuations:\\n{punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d082f99-cfc3-4a15-85b5-c1adc679eda3",
   "metadata": {},
   "source": [
    "<a name=\"2.1.1\"></a>\n",
    "#### Lowercasing, Stopwords, Stemming and Punctuations\n",
    "After initial preprocessing of the data, we only end up with words that contain all the relevant information about the text. Initial preprocesses before splitting and tokenization:\n",
    "* `Lowercasing`: to reduce our vocabulary without losing valuable information, we will have to put each of our words in lowercase. Therefore, the word CHILDREN, Children and children, will be treated as being exactly the same word children.\n",
    "* `Special characters`: such as mathematical symbols, currency symbols, section and paragraph signs, inline markup signs and so on. It's usually safe to delete them.\n",
    "* `Stopwords and punctuation`: we remove all words that do not add significant meaning to the texts, also known as stopwords and punctuation marks. Once eliminated, the general meaning of the sentence can be inferred without any effort.\n",
    "* `Stemming`: is simply transforming any word into its base stem, which we can define as the set of characters used to build the word and its derivatives. The word works for example, its stem is `work`, because, adding the letter \"s\", it forms the word works, adding the suffix \"e\", forms the word worke, and adding the suffix \"ing\", forms the word working.\n",
    "    * After performing stemming on our corpus, the words works, worke and working will be reduced to the stem `work`. Therefore, our vocabulary will be significantly reduced by carrying out this process for each word in the corpus.\n",
    "\n",
    "Preprocessing the data, counting the initially preprocessed and duplicate examples, and plotting the first 5 examples of the initially preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "671c01ed-8e03-4bd0-a7fe-2a160c8153b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate examples: 790\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "2  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "4  shark water x corp shock debut got fenc mend h...  non-action  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data.copy()\n",
    "comics_data_pre['description'] = comics_data_pre['description'].map(rnn_preprocess)\n",
    "print(f'Number of duplicate examples: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78006e44-8e62-4182-8d93-0b3b0affd4b4",
   "metadata": {},
   "source": [
    "Now we can remove the duplicate examples, because they are initially pre-processed and unscored.\n",
    "\n",
    "Plotting the number of duplicates after removing duplicates and plotting the first 5 examples from the pre-processed dataset and without duplicate examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78ea8c4d-3a67-43ad-a3f9-c4abe0e88f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate examples: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93645</td>\n",
       "      <td>Heroes Reborn: Weapon X &amp; Final Flight (2021) #1</td>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93052</td>\n",
       "      <td>Heroes Reborn (2021) #6</td>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             title  \\\n",
       "0  94799            Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339                    The Mighty Valkyries (2021) #3   \n",
       "3  93350                                  X-Corp (2021) #2   \n",
       "5  93645  Heroes Reborn: Weapon X & Final Flight (2021) #1   \n",
       "6  93052                           Heroes Reborn (2021) #6   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "5  best world without aveng squadron suprem prote...  non-action  \n",
       "6  eon fabl daughter utopia isl known power princ...  non-action  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data_pre.drop_duplicates('description')\n",
    "print(f'Number of duplicate examples: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff57126-3376-4b87-a05c-794d4b4003f6",
   "metadata": {},
   "source": [
    "Creating a dataset with only the features that will be used in the model, transforming the target label $y$ into binary and plotting the first 5 examples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd0398d-b2da-4eb2-bd7a-03f1342cd2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  y\n",
       "0  shadow kirisaki mountain secret histori come l...  0\n",
       "1  children afterlif kraven hunter stalk jane fos...  1\n",
       "3  shark water x corp shock debut got fenc mend h...  0\n",
       "5  best world without aveng squadron suprem prote...  0\n",
       "6  eon fabl daughter utopia isl known power princ...  0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the new dataset\n",
    "comics_corpus = comics_data_pre[['description', 'y']].copy()\n",
    "# Transforming the target label y into binary\n",
    "comics_corpus['y'] = comics_corpus['y'].map(lambda x: 1 if x == 'action' else 0)\n",
    "comics_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764098-e7e0-4a26-9a2e-7cc8b4835eba",
   "metadata": {},
   "source": [
    "Dividing the dataset between training, validation and testing subsets. We use the split `stratified sampling` method to try to maintain the same proportion of labels in the division between each subset, because we have a slight imbalance of classes and we don't want this to affect our model.\n",
    "\n",
    "Dividing the dataset and plotting the dimension of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93f68470-6e08-4394-b643-b80f8e183866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (9682, 2)\n",
      "Validation set shape: (3227, 2)\n",
      "Test set shape: (3228, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting between training and the validation and testing subset\n",
    "split_train = StratifiedShuffleSplit(n_splits=1, test_size=.4, random_state=42)\n",
    "for train_index, subset_index in split_train.split(comics_corpus, comics_corpus['y']):\n",
    "    train_corpus, subset_corpus = comics_corpus.iloc[train_index, :].copy(), comics_corpus.iloc[subset_index, :].copy()\n",
    "\n",
    "# Splitting between validation and testing\n",
    "split_test = StratifiedShuffleSplit(n_splits=1, test_size=.5, random_state=42)\n",
    "for val_index, test_index in split_test.split(subset_corpus, subset_corpus['y']):\n",
    "    val_corpus, test_corpus = subset_corpus.iloc[val_index, :].copy(), subset_corpus.iloc[test_index, :].copy()\n",
    "\n",
    "print(f'Train set shape: {train_corpus.shape}\\nValidation set shape: {val_corpus.shape}\\nTest set shape: {test_corpus.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9372fa-bc72-4079-8baf-84b413120439",
   "metadata": {},
   "source": [
    "Setting the global variables `VOCAB_SIZE` and `MAX_LEN` to tokenize the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18056891-e855-4c79-b132-fa93075d407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the largest clean sentence: 166\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "MAX_LEN = max([len(sentence.split()) for sentence in train_corpus['description']])\n",
    "print(f'Length of the largest clean sentence: {MAX_LEN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47db66-5cc9-4174-b0f2-af89b1ca2394",
   "metadata": {},
   "source": [
    "<a name=\"2.1.2\"></a>\n",
    "#### Tokenization\n",
    "In this step, we encode our training set corpus in its vector representation, that is, first we need to create a $V$ vocabulary that allows us to encode any text as a vector of integers, for example. Where our vocabulary $V$ will be a vector of unique words from our vector of texts, where we go through each word of each text and save in the vocabulary all the new words that appear in our search. Then, we map, that is, we replace each word found in the training set with its index in the vocabulary.\n",
    "| Token | Index |\n",
    "| :---: | :---: |\n",
    "| afterlif | 1 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| balanc | 615 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| cosmic | 621 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| work | VOCAB_SIZE |\n",
    "\n",
    "The RNN can receive as input a vector representation of tokenized sentences and then we apply an embedding layer that will transform our tokenized representation of integers into values â€‹â€‹that represent the semantics of that word numerically. This addresses one of the problems, the word order, alphabetical order, in this example, does not make much sense from a semantic point of view. For example, there is no reason 'cosmic' should be given a higher number than 'afterlif'.\n",
    "> I will talk more about embeddings when creating the model.\n",
    "\n",
    "This RNN will allow us to predict sentiment in complex sentences, which we would not be able to correctly classify using simpler methods like Naive Bayes because they miss important information.\n",
    "\n",
    "Our representation $X$ will be a vector of integers, that is, the index of each token in our vocabulary. Once we have all the vector representations of our sentences, we need to identify the maximum vector size and pad each vector with 0 to match that size, this process is called `padding` and it ensures that all of our vectors are the same size, even if our sentences are not.\n",
    "\n",
    "To create vocabulary, we define which words belong to the vocabulary. This means that we create a list of the words that we are going to use in our representations. One way to create this vocabulary is to look at our training set, and find the `VOCAB_SIZE` words with the most occurrence, for example, or we use already created dictionaries that tell us the `VOCAB_SIZE` words most commonly used in the language of our task.\n",
    "\n",
    "In some tasks such as speech recognition, or question answering, we will only find and generate words from a fixed set of words, for example, a chatbot can only answer limited sets of questions. This fixed list of words is also called `closed vocabulary`. However, using a fixed set of words is not always sufficient for the task. Often, we need to deal with words that we have never seen before, which results in an `open vocabulary`. **Open vocabulary** simply means that we can find words outside of vocabulary, like the name of a new city in the training set.\n",
    "\n",
    "If we train a neural network on a corpus of texts based on our vocabulary, when we want to make inference with the trained model, we will need to encode the text we want to infer with the same vocabulary. Otherwise it won't make sense, because the words would map to different numbers, different tokens. Qualquer palavra no corpus de treino que nÃ£o esteja no vocabulÃ¡rio serÃ¡ substituÃ­da por `<UNK>`. Unknown words are also called `Out of vocabulary (OOV)`. Uma forma de lidar com palavras OOV Ã© modelÃ¡-las com uma palavra especial, **\\<UNK>**. Para fazermos isso, substituÃ­mos todas as palavras OOV por \\<UNK>, o special token, `<UNK>`. The proportion of unknown words in the test set is called `OOV rate`.\n",
    "\n",
    "At the end, we apply padding, add 0 to the end of each sequence and make them all the same length. This is called `post-padding`, because the padding tokens are at the end of the sequences.\n",
    "\n",
    "Tensorflow and Keras offer us several ways to tokenize words. One of them is the layer I'm using in this system, [`tensorflow.keras.layers.TextVectorization()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization).\n",
    "* `TextVectorization()`: will generate the vocabulary and create vectors from the sentences. It removes punctuations, that is, it will manage the tokens, transforming the sentences into a list of integers (the indices of each token in the vocabulary) and so on. \n",
    "* `adapt()`: method from the TextVectorization() layer, which takes the data and generates a vocabulary from the words found in these sentences.\n",
    "\n",
    "Training the tokenizer on the training set with the previously defined `VOCAB_SIZE` and `MAX_LEN` and projecting the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cd23d50-369d-488b-9376-c900c09a591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "sentence_vec = rnn_tokenizer(train_corpus['description'], max_tokens=VOCAB_SIZE, max_len=MAX_LEN)\n",
    "print(f'Vocabulary size: {sentence_vec.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7e43b-5e54-43c3-831d-756992450d70",
   "metadata": {},
   "source": [
    "Applying the trained tokenizer to each subset and plotting its dimensions and the first tokenized example of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "671f84aa-426e-4de4-9a4b-c446f81ac9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and padded train sequences shape: (9682, 166)\n",
      "\n",
      "First training padded sequence:\n",
      "[202 101  80  45 332 101  80 804 323 767   1   1 815 649 499 795 303  92\n",
      " 409 624   3   1   1   1   1 413   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "\n",
      "Tokenized and padded validation sequences shape: (3227, 166)\n",
      "Tokenized and padded test sequences shape: (3228, 166)\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = sentence_vec(train_corpus['description'])\n",
    "val_tokenized = sentence_vec(val_corpus['description'])\n",
    "test_tokenized = sentence_vec(test_corpus['description'])\n",
    "print(f'Tokenized and padded train sequences shape: {train_tokenized.shape}\\n\\nFirst training padded sequence:\\n{train_tokenized[0]}\\n')\n",
    "print(f'Tokenized and padded validation sequences shape: {val_tokenized.shape}\\nTokenized and padded test sequences shape: {test_tokenized.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e42e-a0f3-4770-9218-ed41877a2d4f",
   "metadata": {},
   "source": [
    "Loading the trained tokenization model into the `../../models/` directory for later use. We save the hyperparameters that were used in training and the generated vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e20a20-0271-4fa3-9dbf-78da1a2d6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    {'config': sentence_vec.get_config(), 'vocabulary': sentence_vec.get_vocabulary()},\n",
    "    open('../../models/vectorizer.pkl', 'wb')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32031-b879-4358-b2a8-22a7ab39427e",
   "metadata": {},
   "source": [
    "Transforming the $y$ labels into a column vector, concatenating each tokenized corpus with the corresponding $y$ labels and plotting the dimension of each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0738867-7ac3-4312-aff8-26a8eefea2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed train set shape: (9682, 167)\n",
      "Preprocessed validation set shape: (3227, 167)\n",
      "Preprocessed test set shape: (3228, 167)\n"
     ]
    }
   ],
   "source": [
    "# Transforming the y labels into a column vector\n",
    "labels_train = train_corpus[['y']].copy()\n",
    "labels_val = val_corpus[['y']].copy()\n",
    "labels_test = test_corpus[['y']].copy()\n",
    "\n",
    "# Concatenating the corpus of each subset and the corresponding labels\n",
    "train_tokens = np.concatenate([train_tokenized, labels_train], axis=1)\n",
    "val_tokens = np.concatenate([val_tokenized, labels_val], axis=1)\n",
    "test_tokens = np.concatenate([test_tokenized, labels_test], axis=1)\n",
    "print(f'Preprocessed train set shape: {train_tokens.shape}\\nPreprocessed validation set shape: {val_tokens.shape}\\nPreprocessed test set shape: {test_tokens.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af5b1-21f5-411a-bfbf-77385bdf293f",
   "metadata": {},
   "source": [
    "Loading each preprocessed dataset into the `../../data/preprocessed/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5444967e-b6d0-4877-853c-10d2e41d6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset with initial pre-processing to disk\n",
    "comics_corpus.to_csv('../../data/preprocessed/comics_corpus.csv', index=False)\n",
    "\n",
    "# Loading tokenized datasets to disk\n",
    "np.save('../../data/preprocessed/train_tokens.npy', train_tokens)\n",
    "np.save('../../data/preprocessed/validation_tokens.npy', val_tokens)\n",
    "np.save('../../data/preprocessed/test_tokens.npy', test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60216a38-23ab-4aad-bc3f-99f5195e81ad",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### Transformer Preprocessing\n",
    "For transformers preprocessing, we will use a pre-trained tokenizer from the `DistilBERT` checkpoint to apply tokenization and padding.\n",
    "\n",
    "DistilBERT is a small, fast, cheap and lightweight Transformers model trained by distilling the BERT (Bidirectional Encoder Representation from Transformers) base model. It has 40% fewer parameters than bert-base-uncased, runs 60% faster, and preserves more than 95% of Bert's performance as measured in the GLUE (General Language Understanding Evaluation) benchmark.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (ðŸ¤—) is the best resource for pre-trained transformers. Its open-source libraries make it simple to download, fine-tune, and use transformer models like DeepSeek, BERT, Llama, T5, Qwen, GPT-2, and more. And the best part, you can use them together with TensorFlow, PyTorch or Flax. In this system, I use ðŸ¤— transformers to use the `DistilBERT` model for sentiment classification. For the pre-processing step, we used the pre-trained DistilBERT tokenizer `distilbert-base-uncased-finetuned-sst-2-english`, for this we initialized the DistilBertTokenizer class and defined the desired pre-trained model.\n",
    "> In the fine-tuning of the model, in the notebook `05_transformers_finetuning.ipynb`, I will talk in more detail about transfer learning and fine-tuning.\n",
    "\n",
    "Tokenizing, padding the corpus and returning the tokenized corpus as pytorch tensors using the pre-trained `DistilBERT` tokenizer. Defining the tokenizer and plotting the vector representations of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4600970c-28f4-4ea8-9ffb-a28b511145cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1999,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1037,  3181,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  2028,  ...,     0,     0,     0],\n",
       "        [  101, 16228,  2023,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the pre-trained model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "# Setting the tokenizer\n",
    "comics_transformers = tokenizer(\n",
    "    comics_data['description'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "# Accessing the vector representations of the corpus\n",
    "transformers_tokens = comics_transformers['input_ids']\n",
    "transformers_attention = comics_transformers['attention_mask']\n",
    "transformers_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7d949-f92d-47ea-b8f2-b004af97ba74",
   "metadata": {},
   "source": [
    "Selecting the labels from the raw dataset and dividing the tokenized tensor between the training, validation and testing subsets. We use the `stratified sampling` split method to try to maintain the same proportion of labels in the division between each subset.\n",
    "\n",
    "Dividing the dataset and plotting the size of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97f49488-f556-47c9-bcb9-33ac7988ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset size: 10156\n",
      "Validation subset size: 3385\n",
      "Test subset size: 3386\n"
     ]
    }
   ],
   "source": [
    "# Selecting labels from the raw dataset\n",
    "labels = (comics_data['y']\n",
    "          .map(lambda x: 1 if x == 'action' else 0)\n",
    "          .to_numpy()\n",
    "          .reshape(-1, 1))\n",
    "\n",
    "# Splitting between training and the validation and testing subset\n",
    "train_idx, subset_idx = next(split_train.split(transformers_tokens, labels))\n",
    "# Splitting between validation and testing\n",
    "val_idx, test_idx = next(split_test.split(transformers_tokens[subset_idx], labels[subset_idx]))\n",
    "print(f'Train subset size: {len(train_idx)}\\nValidation subset size: {len(val_idx)}\\nTest subset size: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba41bb9-9768-4121-8ebc-e982a80102b8",
   "metadata": {},
   "source": [
    "Transforming the tokenized pytorch tensor subsets divided by the `stratified sampling split` to the `Dataset` type in dictionary format, to perform fine-tuning of the transformers and plotting each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b657d0b8-5c4c-413e-aa4c-2aa37c8e134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Validation dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, train_idx)\n",
    "val_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, val_idx)\n",
    "test_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, test_idx)\n",
    "print(f'Train dataset: {train_dataset}\\nValidation dataset: {val_dataset}\\nTest dataset: {test_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7634f8-e5e5-4a58-bbd0-289af0787473",
   "metadata": {},
   "source": [
    "Loading each pre-processed dataset and its metadata into its specific directory within the `../../data/preprocessed/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d767741-02a8-493a-a30d-f1ebec36b2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3278c9f7c64bb88eded1df5b529eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf2b8fe7fed4753b4811cd6fb13f290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7784715774b4fde89c017ddbf2206fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('../../data/preprocessed/train_dataset')\n",
    "val_dataset.save_to_disk('../../data/preprocessed/validation_dataset')\n",
    "test_dataset.save_to_disk('../../data/preprocessed/test_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
