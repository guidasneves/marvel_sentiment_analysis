{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debe9ec0-89c7-46ab-9203-b210db8004cc",
   "metadata": {},
   "source": [
    "# Etapa do Pré-Processamento\n",
    "Etapa do pré-processamento dos dados para depois, alimentarmos esses dados ao modelo.\n",
    "\n",
    "Nesta etapa realizamos:\n",
    "* Transformação de todas as palavras para minúsculas;\n",
    "* Remoção de pontuações;\n",
    "* Remoção de stopwords;\n",
    "* Aplicamos o stemming;\n",
    "* Divisão entre treino, validação e teste;\n",
    "* Tokenização;\n",
    "* Padding.\n",
    "\n",
    "> **Nota**: **Artigo no Medium** da etapa do `pré-processemanto` desse sistema em português: [Análise de Sentimentos Sobre os Quadrinhos da Marvel (Parte 1) - Ingestão, EDA e Pré-processamento](https://medium.com/@guineves.py/c5a0e35bb586)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa711489-0961-4df2-af39-63583899af35",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Pacotes](#1)\n",
    "* [Pré-processamento](#2)\n",
    "    * [Pré-processamento da RNN](#2.1)\n",
    "        * [Lowercasing, Stopwords, Stemming e Pontuações](#2.1.1)\n",
    "        * [Tokenização](#2.1.2)\n",
    "    * [Pré-processamento do Transformer](#2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec45f-ccfd-46c9-913b-601cf152c125",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Packages (Pacotes)\n",
    "Pacotes que foram utilizados no sistema:\n",
    "* [pandas](https://pandas.pydata.org/): é o principal pacote para manipulação de dados;\n",
    "* [numpy](www.numpy.org): é o principal pacote para computação científica;\n",
    "* [re](https://docs.python.org/3/library/re.html): fornece operações de correspondência de expressões regulares semelhantes às encontradas em Perl;\n",
    "* [string](https://docs.python.org/pt-br/3.13/library/string.html): para operações comuns de strings;\n",
    "* [nltk](https://www.nltk.org/): NLTK é uma plataforma líder para a construção de programas Python para trabalhar com dados de linguagem humana;\n",
    "* [tensorflow](https://www.tensorflow.org/): framework que facilita a criação de modelos de machine learning que podem ser executados em qualquer ambiente;\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): biblioteca open-source de machine learning;\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html): implementa protocolos binários para serializar e desserializar uma estrutura de objeto Python;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): fornece APIs e ferramentas para baixar e treinar facilmente modelos pré-treinados de última geração;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): é uma biblioteca para acessar e compartilhar facilmente datasets para tarefas de áudio, visão computacional e processamento de linguagem natural (NLP);\n",
    "* [os](https://docs.python.org/3/library/os.html): módulo integrado, fornece uma maneira portátil de usar funcionalidades dependentes do sistema operacional;\n",
    "* [sys](https://docs.python.org/3/library/sys.html): fornece acesso a algumas variáveis usadas ou mantidas pelo interpretador e a funções que interagem fortemente com o interpretador;\n",
    "* [src](../src/): pacote com todos os códigos de todas as funções utilitárias criadas para esse sistema. Localizado dentro do diretório `../src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "375d1727-909a-4086-ba17-f377c05c39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Obtendo a versão absoluta normalizada do path raíz do projeto\n",
    "    os.path.join( # Concatenando os paths\n",
    "        os.getcwd(), # Obtendo o path do diretório dos notebooks\n",
    "        os.pardir # Obtendo a string constante usada pelo OS para fazer referência ao diretório pai\n",
    "    )\n",
    ")\n",
    "# Adicionando o path à lista de strings que especifica o path de pesquisa para os módulos\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb33b25-3901-447b-9a3d-3aaaf12f60ad",
   "metadata": {},
   "source": [
    "> **Nota**: os códigos para as funções utilitárias utilizadas nesse sistema estão no script `preprocessing.py` dentro do diretório `../src/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e78a5-0cc8-4cd6-b8f2-4373e4035379",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Pré-processamento\n",
    "Vamos criar 2 modelos para esse projeto. Portanto, faremos 2 pré-processamentos diferentes, um para cada modelo:\n",
    "1. O primeiro será utilizando uma RNN com uma layer LSTM bidirecional criadas e treinada do zero.\n",
    "2. O segundo modelo será a aplicação de fine-tuning em um modelo com arquitetura transformers pré-treinado.\n",
    "\n",
    "Lendo o dataset que será pré-processado e projetando os seus primeiros 5 exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e195ab81-50b4-4e6a-8eb9-0baaa1431269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP’s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP?s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...  non-action  \n",
       "1  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "2  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "3  A SHARK IN THE WATER! After X-CORP’s shocking ...  non-action  \n",
       "4  A SHARK IN THE WATER! After X-CORP?s shocking ...  non-action  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data = pd.read_csv('../data/raw/comics_corpus.csv')\n",
    "comics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a77c2-c521-447b-ad4a-3db973b379b6",
   "metadata": {},
   "source": [
    "Podemos ver algumas frases duplicadas na feature `description`. Mas se explorarmos mais de perto, podemos ver que o que não faz elas se tornarem 100% similares, são as pontuações. Portanto, vamos tratar as pontuações para eliminar os exemplos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f25a5f4-ec2e-4758-b71c-95fd9107f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algum exemplo:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn’t Hel’s only ruler—and now she’s upset the cosmic balance. There will be a price to pay…and Karnilla intends to ensure the Valkyries pay it.\n",
      "\n",
      "Exemplo duplicado:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn?t Hel?s only ruler?and now she?s upset the cosmic balance. There will be a price to pay?and Karnilla intends to ensure the Valkyries pay it.\n"
     ]
    }
   ],
   "source": [
    "print(f'Algum exemplo:\\n{comics_data[\"description\"][1]}\\n')\n",
    "print(f'Exemplo duplicado:\\n{comics_data[\"description\"][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957d92e-6fad-41e5-b368-00a1dca68cbb",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### Pré-processamento da RNN\n",
    "Para um classificador de sentimentos, primeiro pré-processamos os dados brutos, depois, tokenizamos o nosso train set e extraímos as features úteis para treinarmos o nosso modelo e fazermos nossas previsões.\n",
    "\n",
    "Plotando as stopwords em ingês e as pontuações que serão removidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61015b4e-cfc4-4c92-b2cb-0675e5125239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Pontuações:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "print(f'Stopwords:\\n{stopwords_en}\\n\\nPontuações:\\n{punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d082f99-cfc3-4a15-85b5-c1adc679eda3",
   "metadata": {},
   "source": [
    "<a name=\"2.1.1\"></a>\n",
    "#### Lowercasing, Stopwords, Stemming e Pontuações\n",
    "Após o pré-processamento inicial dos dados, acabamos apenas com as palavras que contêm todas as informações relevantes sobre o texto. Pré-processamentos iniciais antes da divisão e tokenização:\n",
    "* `Lowercasing`: para reduzir nosso vocabulário sem perder informações valiosas, teremos que colocar cada uma de nossas palavras em lowercase. Portanto, a palavra CHILDREN, Children e children, serão tratadas como sendo exatamente a mesma palavra children.\n",
    "* `Caracteres especiais`: como símbolos matemáticos, símbolos monetários, sinais de seção e parágrafo, sinais de marcação inline e assim por diante. Geralmente é seguro excluí-los.\n",
    "* `Stopwords e pontuações`: removemos todas as palavras que não acrescentam significado significativo aos textos, também conhecidas como stopwords e sinais de pontuação (punctuation marks). Após eliminadas, o significado geral da frase pode ser inferido sem nenhum esforço.\n",
    "* `Stemming`: é simplesmente transformar qualquer palavra em sua base stem (raiz base), que podemos definir como o set de caracteres usados para construir a palavra e seus derivados. A palavra works por exemplo, seu stem é `work`, porque, adicionando a letra \"s\", forma a palavra works, adicionando o sufixo \"e\", forma a palavra worke, e adicionando o sufixo \"ing\", forma a palavra working.\n",
    "    * Depois de executar a stemming no nosso corpus, as palavras works, worke e working, serão reduzidas para a stem `work`. Portanto, nosso vocabulário será reduzido significativamente ao realizar esse processo para cada palavra do corpus.\n",
    "\n",
    "Pré-processando os dados, contando os exemplos inicialmente pré-processados e duplicados, e plotando os primeiros 5 primeiros exemplos do dataset inicialmente pré-processado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "671c01ed-8e03-4bd0-a7fe-2a160c8153b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de exemplos duplicados: 790\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "2  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "4  shark water x corp shock debut got fenc mend h...  non-action  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data.copy()\n",
    "comics_data_pre['description'] = comics_data_pre['description'].map(rnn_preprocess)\n",
    "print(f'Número de exemplos duplicados: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78006e44-8e62-4182-8d93-0b3b0affd4b4",
   "metadata": {},
   "source": [
    "Agora podemos remover os exemplos duplicados, porque estão inicialmente pré-processados e sem pontuações.\n",
    "\n",
    "Plotando o número de duplicatas após a removação das duplicatas e plotando os primeiros 5 exemplos do dataset pré-processado e sem exemplos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78ea8c4d-3a67-43ad-a3f9-c4abe0e88f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de exemplos duplicados: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93645</td>\n",
       "      <td>Heroes Reborn: Weapon X &amp; Final Flight (2021) #1</td>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93052</td>\n",
       "      <td>Heroes Reborn (2021) #6</td>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             title  \\\n",
       "0  94799            Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339                    The Mighty Valkyries (2021) #3   \n",
       "3  93350                                  X-Corp (2021) #2   \n",
       "5  93645  Heroes Reborn: Weapon X & Final Flight (2021) #1   \n",
       "6  93052                           Heroes Reborn (2021) #6   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "5  best world without aveng squadron suprem prote...  non-action  \n",
       "6  eon fabl daughter utopia isl known power princ...  non-action  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data_pre.drop_duplicates('description')\n",
    "print(f'Número de exemplos duplicados: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff57126-3376-4b87-a05c-794d4b4003f6",
   "metadata": {},
   "source": [
    "Criando um dataset com apenas as features que serão usadas no modelo, transformando o target label $y$ em binário e plotando os primeiros 5 exemplos do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cd0398d-b2da-4eb2-bd7a-03f1342cd2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  y\n",
       "0  shadow kirisaki mountain secret histori come l...  0\n",
       "1  children afterlif kraven hunter stalk jane fos...  1\n",
       "3  shark water x corp shock debut got fenc mend h...  0\n",
       "5  best world without aveng squadron suprem prote...  0\n",
       "6  eon fabl daughter utopia isl known power princ...  0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o novo dataset\n",
    "comics_corpus = comics_data_pre[['description', 'y']].copy()\n",
    "# Transformando o target label y em binário\n",
    "comics_corpus['y'] = comics_corpus['y'].map(lambda x: 1 if x == 'action' else 0)\n",
    "comics_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764098-e7e0-4a26-9a2e-7cc8b4835eba",
   "metadata": {},
   "source": [
    "Dividindo o dataset entre os subsets de treinamento, de validação e de teste. Usamos o split método `stratified sampling (amostragem estratificada)` para tentarmos manter a mesma proporção dos labels na divisão entre cada subset, porque temos um leve desbalanceamento de classes e não queremos que isso afete o nosso modelo.\n",
    "\n",
    "Dividindo o dataset e plotando a dimensão de cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93f68470-6e08-4394-b643-b80f8e183866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do subset de treinamento: (9682, 2)\n",
      "Shape do subset de valiação: (3227, 2)\n",
      "Shape do subset de teste: (3228, 2)\n"
     ]
    }
   ],
   "source": [
    "# Dividindo entre treinamento e o subset da validação e teste\n",
    "split_train = StratifiedShuffleSplit(n_splits=1, test_size=.4, random_state=42)\n",
    "for train_index, subset_index in split_train.split(comics_corpus, comics_corpus['y']):\n",
    "    train_corpus, subset_corpus = comics_corpus.iloc[train_index, :].copy(), comics_corpus.iloc[subset_index, :].copy()\n",
    "\n",
    "# Dividindo entre validação e teste\n",
    "split_test = StratifiedShuffleSplit(n_splits=1, test_size=.5, random_state=42)\n",
    "for val_index, test_index in split_test.split(subset_corpus, subset_corpus['y']):\n",
    "    val_corpus, test_corpus = subset_corpus.iloc[val_index, :].copy(), subset_corpus.iloc[test_index, :].copy()\n",
    "\n",
    "print(f'Shape do subset de treinamento: {train_corpus.shape}\\nShape do subset de valiação: {val_corpus.shape}\\nShape do subset de teste: {test_corpus.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9372fa-bc72-4079-8baf-84b413120439",
   "metadata": {},
   "source": [
    "Definindo as variáveis globais `VOCAB_SIZE` e `MAX_LEN` para tokenizar o training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18056891-e855-4c79-b132-fa93075d407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da maior sequência limpa: 166\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "MAX_LEN = max([len(sentence.split()) for sentence in train_corpus['description']])\n",
    "print(f'Tamanho da maior sequência limpa: {MAX_LEN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47db66-5cc9-4174-b0f2-af89b1ca2394",
   "metadata": {},
   "source": [
    "<a name=\"2.1.2\"></a>\n",
    "#### Tokenização\n",
    "Nessa etapa, codificamos o nosso corpus do training set em sua representação vetorial, ou seja, primeiro precisamos criar um vocabulário $V$ que nos permite codificar qualquer texto como um vetor de inteiros, por exemplo. Onde o nosso vocabulário $V$ será um vetor de palavras exclusivas do nosso vetor de textos, onde percorremos cada palavra de cada texto e salvamos no vocabulário todas as novas palavras que aparecerem em nossa pesquisa. Então, mapeamos, ou seja, substituímos cada palavra encontrada no training set pelo seu índice no vocabulário.\n",
    "| Token | Índice |\n",
    "| :---: | :----: |\n",
    "| afterlif | 1 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| balanc | 615 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| cosmic | 621 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| work | VOCAB_SIZE |\n",
    "\n",
    "A RNN pode receber como input uma representação vetorial de frases tokenizadas e, então, aplicamos uma embedding layer que transformará nossa representação tokenizada de inteiros em valores que representam a semântica dessa palavra numéricamente. Isso trata um dos problemas, a ordem das palavras, ordem alfabética, neste exemplo, não faz muito sentido do ponto de vista semântico. Por exemplo, não há razão para que o 'cosmic' receba um número maior do que o 'afterlif'.\n",
    "> Falarei mais sobre embeddings na criação do modelo.\n",
    "\n",
    "Essa RNN nos permitirá prever sentimentos em frases complexas, que não conseguiríamos classificar corretamente usando métodos mais simples, como Naive Bayes, porque eles perdem informações importantes.\n",
    "\n",
    "A nossa representação $X$ será um vetor de números inteiros, ou seja, o índice de cada token do nosso vocabulário. Depois de obtermos todas as representações vetoriais das nossas frases, precisamos identificar o tamanho máximo do vetor e preencher cada vetor com 0 para corresponder a essa tamanho, esse processo é chamado de `padding` e garante que todos os nossos vetores tenham o mesmo tamanho, mesmo que nossas frases não tenham.\n",
    "\n",
    "Para a criação do vocabulário, definimos quais palavras pertencem ao vocabulário. Isso significa que criamos uma lista com as palavras que vamos usar nas nossas representações. Uma forma de criar esse vocabulário é olhar para o nosso training set, e encontrar as `VOCAB_SIZE` palavras com mais ocorrência, por exemplo, ou utilizamos dicionários já criados que nos digam as `VOCAB_SIZE` palavras mais comumente usadas na língua da nossa tarefa.\n",
    "\n",
    "Em algumas tarefas como speech recognition, ou question aswering, encontraremos e geraremos somente palavras a partir de um set fixo de palavras, por exemplo, um chatbot só pode responder em sets limitados de perguntas. Essa lista fixa de palavras também é chamada de `closed vocabulary`. No entanto, usar um set fixo de palavras nem sempre é suficiente para a tarefa. Muitas vezes, precisamos lidar com palavras que nunca vimos antes, o que resulta em um `open vocabulary`. **Open vocabulary** significa simplesmente que podemos encontrar palavras out of vocabulary (fora do vocabulário), como o nome de uma nova cidade no training set.\n",
    "\n",
    "Se treinarmos uma rede neural em um corpus de textos com base nesse nosso vocabulário, quando quisermos fazer inferência com o modelo treinado, precisaremos codificar o texto que desejamos inferir com o mesmo vocabulário. Caso contrário, não fará sentido, porque as palavras seriam mapeadas para números diferentes, tokens diferentes. Qualquer palavra no corpus de treino que não esteja no vocabulário será substituída por `<UNK>`. As unknown words também são chamadas de `Out of vocabulary (OOV)`. Uma forma de lidar com palavras OOV é modelá-las com uma palavra especial, **\\<UNK>**. Para fazermos isso, substituímos todas as palavras OOV por \\<UNK>, o special token, `<UNK>`. A proporção de unknown words no test set é chamada de `OOV rate`.\n",
    "\n",
    "No final, aplicamos o `padding`, adicionamos 0 ao final de cada sequence e fazemos com que todas tenham o mesmo comprimento. Isso é chamado de `post-padding (pós-padding)`, porque os tokens de padding estão no final das sequências.\n",
    "\n",
    "O tensorflow e o Keras nos oferecem várias maneiras de tokenizar palavras. Uma delas é a layer que estou usando nesse sistema, [`tensorflow.keras.layers.TextVectorization()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization).\n",
    "* `TextVectorization()`: gerará o vocabulário e criará vetores a partir das frases. Ela retira pontuações, ou seja, ela irá gerenciar os tokens, transformando as frases em uma lista de inteiros (os índices de cada token no vocabulário) e etc. \n",
    "* `adapt()`: método da layer TextVectorization(), que pega os dados e gera um vocabulário a partir das palavras encontradas nessas frases.\n",
    "\n",
    "Treinando o tokenizador no training set com o `VOCAB_SIZE` e `MAX_LEN` definidos anteriormente e projetando o tamanho do vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4cd23d50-369d-488b-9376-c900c09a591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 1000\n"
     ]
    }
   ],
   "source": [
    "sentence_vec = rnn_tokenizer(train_corpus['description'], max_tokens=VOCAB_SIZE, max_len=MAX_LEN)\n",
    "print(f'Tamanho do vocabulário: {sentence_vec.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7e43b-5e54-43c3-831d-756992450d70",
   "metadata": {},
   "source": [
    "Aplicando o tokenizador treinado em cada subset e plotando as suas dimensões e o primeiro exemplo tokenizado do training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "671f84aa-426e-4de4-9a4b-c446f81ac9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão das sequências de treino tokenizadas e com padding: (9682, 166)\n",
      "\n",
      "Primeira sequência de treino com padding:\n",
      "[202 101  80  45 332 101  80 804 323 767   1   1 815 649 499 795 303  92\n",
      " 409 624   3   1   1   1   1 413   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "\n",
      "Dimensão das sequências de validação tokenizadas e com padding: (3227, 166)\n",
      "Dimensão das sequências de teste tokenizadas e com padding: (3228, 166)\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = sentence_vec(train_corpus['description'])\n",
    "val_tokenized = sentence_vec(val_corpus['description'])\n",
    "test_tokenized = sentence_vec(test_corpus['description'])\n",
    "print(f'Dimensão das sequências de treino tokenizadas e com padding: {train_tokenized.shape}\\n\\nPrimeira sequência de treino com padding:\\n{train_tokenized[0]}\\n')\n",
    "print(f'Dimensão das sequências de validação tokenizadas e com padding: {val_tokenized.shape}')\n",
    "print(f'Dimensão das sequências de teste tokenizadas e com padding: {test_tokenized.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e42e-a0f3-4770-9218-ed41877a2d4f",
   "metadata": {},
   "source": [
    "Carregando o modelo de tokenização treinado no diretório `../models/` para usarmos posteriormente. Salvamos os hiperparâmetros que foram utilizados no treinamento e o vocabulário gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e20a20-0271-4fa3-9dbf-78da1a2d6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    {'config': sentence_vec.get_config(), 'vocabulary': sentence_vec.get_vocabulary()},\n",
    "    open('../models/vectorizer.pkl', 'wb')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32031-b879-4358-b2a8-22a7ab39427e",
   "metadata": {},
   "source": [
    "Transformando os labels $y$ em um vetor de coluna, concatenando cada corpus tokenizado com os labels $y$ correspondentes e plotando a dimensão de cada subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0738867-7ac3-4312-aff8-26a8eefea2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão do subset de treino pré-processado: (9682, 167)\n",
      "Dimensão do subset de validação pré-processado: (3227, 167)\n",
      "Dimensão do subset de teste pré-processado: (3228, 167)\n"
     ]
    }
   ],
   "source": [
    "# Transformando os labels y em um vetor de coluna\n",
    "labels_train = train_corpus[['y']].copy()\n",
    "labels_val = val_corpus[['y']].copy()\n",
    "labels_test = test_corpus[['y']].copy()\n",
    "\n",
    "# Concatenando o corpus de cada subset e os labels correspondentes\n",
    "train_tokens = np.concatenate([train_tokenized, labels_train], axis=1)\n",
    "val_tokens = np.concatenate([val_tokenized, labels_val], axis=1)\n",
    "test_tokens = np.concatenate([test_tokenized, labels_test], axis=1)\n",
    "print(f'Dimensão do subset de treino pré-processado: {train_tokens.shape}')\n",
    "print(f'Dimensão do subset de validação pré-processado: {val_tokens.shape}')\n",
    "print(f'Dimensão do subset de teste pré-processado: {test_tokens.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af5b1-21f5-411a-bfbf-77385bdf293f",
   "metadata": {},
   "source": [
    "Carregando cada dataset pré-processado no diretório `../data/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5444967e-b6d0-4877-853c-10d2e41d6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando no disco o dataset com pré-processamento inicial\n",
    "comics_corpus.to_csv('../data/preprocessed/comics_corpus.csv', index=False)\n",
    "# Carregando no disco os datasets tokenizados\n",
    "np.save('../data/preprocessed/train_tokens.npy', train_tokens)\n",
    "np.save('../data/preprocessed/validation_tokens.npy', val_tokens)\n",
    "np.save('../data/preprocessed/test_tokens.npy', test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60216a38-23ab-4aad-bc3f-99f5195e81ad",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### Pré-processamento do Transformers\n",
    "Para o pré-processamento do transformers, vamos utilizar um tokenizador pré-treinado do checkpoint do `DistilBERT` para aplicar a tokenização e o padding.\n",
    "\n",
    "DistilBERT é um modelo Transformers pequeno, rápido, barato e leve treinado pela destilação do modelo base BERT (Bidirectional Encoder Representation from Transformers). Ele tem 40% menos parâmetros que o bert-base-uncased, roda 60% mais rápido e preserva mais de 95% do desempenho do Bert conforme medido no benchmark GLUE (General Language Understanding Evaluation).\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (🤗) é o melhor recurso para transformers pré-treinados. Suas bibliotecas open-source simplificam o download, o fine-tuning e o uso de modelos de transformers como DeepSeek, BERT, Llama, T5, Qwen, GPT-2 e muito mais. E a melhor parte, você pode usá-los junto com TensorFlow, PyTorch ou Flax. Neste sistema, utilizo transformers 🤗 para usar o modelo `DistilBERT` para classificação de sentimento. Para a etapa do pré-processamento, usamos o tokenizador do DistilBERT `distilbert-base-uncased-finetuned-sst-2-english` pré-treinado, para isso inicializamos a classe DistilBertTokenizer e definidos o modelo pré-treinado desejado.\n",
    "> No fine-tuning do modelo, no notebook `05_transformers_finetuning.ipynb`, falarei com mais detalhes sobre o transfer learning e o fine-tuning.\n",
    "\n",
    "Tokenizando, aplicando o padding no corpus e retornando o corpus tokenizado como tensores pytorch utilizando o tokenizer `DistilBERT` pré-treinado. Definindo o tokenizer e plotando as representações vetoriais do corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4600970c-28f4-4ea8-9ffb-a28b511145cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1999,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1037,  3181,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  2028,  ...,     0,     0,     0],\n",
       "        [  101, 16228,  2023,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o modelo pré-treinado\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "# Definindo o tokenizer\n",
    "comics_transformers = tokenizer(\n",
    "    comics_data['description'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "# Acessando as representações vetoriais do corpus\n",
    "transformers_tokens = comics_transformers['input_ids']\n",
    "transformers_attention = comics_transformers['attention_mask']\n",
    "transformers_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7d949-f92d-47ea-b8f2-b004af97ba74",
   "metadata": {},
   "source": [
    "Selecionando os labels do dataset bruto e dividindo o tensor tokenizado entre os subsets de treinamento, de validação e de teste. Usamos o método de split `stratified sampling (amostragem estratificada)` para tentarmos manter a mesma proporção dos labels na divisão entre cada subset.\n",
    "\n",
    "Dividindo o dataset e plotando o tamanho de cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97f49488-f556-47c9-bcb9-33ac7988ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do subset de treino: 10156\n",
      "Tamanho do subset de validação: 3385\n",
      "Tamanho do subset de teste: 3386\n"
     ]
    }
   ],
   "source": [
    "# Selecionando os labels do dataset bruto\n",
    "labels = (comics_data['y']\n",
    "          .map(lambda x: 1 if x == 'action' else 0)\n",
    "          .to_numpy()\n",
    "          .reshape(-1, 1))\n",
    "\n",
    "# Dividindo entre treinamento e o subset da validação e teste\n",
    "train_idx, subset_idx = next(split_train.split(transformers_tokens, labels))\n",
    "# Dividindo entre validação e teste\n",
    "val_idx, test_idx = next(split_test.split(transformers_tokens[subset_idx], labels[subset_idx]))\n",
    "print(f'Tamanho do subset de treino: {len(train_idx)}\\nTamanho do subset de validação: {len(val_idx)}\\nTamanho do subset de teste: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba41bb9-9768-4121-8ebc-e982a80102b8",
   "metadata": {},
   "source": [
    "Transformando os subsets de tensores pytorch tokenizados divididos pela `stratified sampling split` para o tipo `Dataset` em formato de dicionário, para executarmos o fine-tuning do transformers e plotando cada dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b657d0b8-5c4c-413e-aa4c-2aa37c8e134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Dataset de validação: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Dataset de teste: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, train_idx)\n",
    "val_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, val_idx)\n",
    "test_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, test_idx)\n",
    "print(f'Dataset de treino: {train_dataset}\\nDataset de validação: {val_dataset}\\nDataset de teste: {test_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7634f8-e5e5-4a58-bbd0-289af0787473",
   "metadata": {},
   "source": [
    "Carregando cada dataset pré-processado e seus metadados dentro de seu diretório específico dentro do diretório `../data/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d767741-02a8-493a-a30d-f1ebec36b2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3278c9f7c64bb88eded1df5b529eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf2b8fe7fed4753b4811cd6fb13f290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7784715774b4fde89c017ddbf2206fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('../data/preprocessed/train_dataset')\n",
    "val_dataset.save_to_disk('../data/preprocessed/validation_dataset')\n",
    "test_dataset.save_to_disk('../data/preprocessed/test_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
