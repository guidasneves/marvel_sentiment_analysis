{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debe9ec0-89c7-46ab-9203-b210db8004cc",
   "metadata": {},
   "source": [
    "# Etapa do Pr√©-Processamento\n",
    "Etapa do pr√©-processamento dos dados para depois, alimentarmos esses dados ao modelo.\n",
    "\n",
    "Nesta etapa realizamos:\n",
    "* Transforma√ß√£o de todas as palavras para min√∫sculas;\n",
    "* Remo√ß√£o de pontua√ß√µes;\n",
    "* Remo√ß√£o de stopwords;\n",
    "* Aplicamos o stemming;\n",
    "* Divis√£o entre treino, valida√ß√£o e teste;\n",
    "* Tokeniza√ß√£o;\n",
    "* Padding.\n",
    "\n",
    "> **Nota**: **Artigo no Medium** da etapa do `pr√©-processemanto` desse sistema em portugu√™s: [An√°lise de Sentimentos Sobre os Quadrinhos da Marvel (Parte 1)‚Ää-‚ÄäIngest√£o, EDA e Pr√©-processamento](https://medium.com/@guineves.py/c5a0e35bb586)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa711489-0961-4df2-af39-63583899af35",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Pacotes](#1)\n",
    "* [Pr√©-processamento](#2)\n",
    "    * [Pr√©-processamento da RNN](#2.1)\n",
    "        * [Lowercasing, Stopwords, Stemming e Pontua√ß√µes](#2.1.1)\n",
    "        * [Tokeniza√ß√£o](#2.1.2)\n",
    "    * [Pr√©-processamento do Transformer](#2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec45f-ccfd-46c9-913b-601cf152c125",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Packages (Pacotes)\n",
    "Pacotes que foram utilizados no sistema:\n",
    "* [pandas](https://pandas.pydata.org/): √© o principal pacote para manipula√ß√£o de dados;\n",
    "* [numpy](www.numpy.org): √© o principal pacote para computa√ß√£o cient√≠fica;\n",
    "* [re](https://docs.python.org/3/library/re.html): fornece opera√ß√µes de correspond√™ncia de express√µes regulares semelhantes √†s encontradas em Perl;\n",
    "* [string](https://docs.python.org/pt-br/3.13/library/string.html): para opera√ß√µes comuns de strings;\n",
    "* [nltk](https://www.nltk.org/): NLTK √© uma plataforma l√≠der para a constru√ß√£o de programas Python para trabalhar com dados de linguagem humana;\n",
    "* [tensorflow](https://www.tensorflow.org/): framework que facilita a cria√ß√£o de modelos de machine learning que podem ser executados em qualquer ambiente;\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): biblioteca open-source de machine learning;\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html): implementa protocolos bin√°rios para serializar e desserializar uma estrutura de objeto Python;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): fornece APIs e ferramentas para baixar e treinar facilmente modelos pr√©-treinados de √∫ltima gera√ß√£o;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): √© uma biblioteca para acessar e compartilhar facilmente datasets para tarefas de √°udio, vis√£o computacional e processamento de linguagem natural (NLP);\n",
    "* [os](https://docs.python.org/3/library/os.html): m√≥dulo integrado, fornece uma maneira port√°til de usar funcionalidades dependentes do sistema operacional;\n",
    "* [sys](https://docs.python.org/3/library/sys.html): fornece acesso a algumas vari√°veis usadas ou mantidas pelo interpretador e a fun√ß√µes que interagem fortemente com o interpretador;\n",
    "* [src](../src/): pacote com todos os c√≥digos de todas as fun√ß√µes utilit√°rias criadas para esse sistema. Localizado dentro do diret√≥rio `../src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "375d1727-909a-4086-ba17-f377c05c39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Obtendo a vers√£o absoluta normalizada do path ra√≠z do projeto\n",
    "    os.path.join( # Concatenando os paths\n",
    "        os.getcwd(), # Obtendo o path do diret√≥rio dos notebooks\n",
    "        os.pardir # Obtendo a string constante usada pelo OS para fazer refer√™ncia ao diret√≥rio pai\n",
    "    )\n",
    ")\n",
    "# Adicionando o path √† lista de strings que especifica o path de pesquisa para os m√≥dulos\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb33b25-3901-447b-9a3d-3aaaf12f60ad",
   "metadata": {},
   "source": [
    "> **Nota**: os c√≥digos para as fun√ß√µes utilit√°rias utilizadas nesse sistema est√£o no script `preprocessing.py` dentro do diret√≥rio `../src/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e78a5-0cc8-4cd6-b8f2-4373e4035379",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Pr√©-processamento\n",
    "Vamos criar 2 modelos para esse projeto. Portanto, faremos 2 pr√©-processamentos diferentes, um para cada modelo:\n",
    "1. O primeiro ser√° utilizando uma RNN com uma layer LSTM bidirecional criadas e treinada do zero.\n",
    "2. O segundo modelo ser√° a aplica√ß√£o de fine-tuning em um modelo com arquitetura transformers pr√©-treinado.\n",
    "\n",
    "Lendo o dataset que ser√° pr√©-processado e projetando os seus primeiros 5 exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e195ab81-50b4-4e6a-8eb9-0baaa1431269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP‚Äôs shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP?s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...  non-action  \n",
       "1  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "2  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "3  A SHARK IN THE WATER! After X-CORP‚Äôs shocking ...  non-action  \n",
       "4  A SHARK IN THE WATER! After X-CORP?s shocking ...  non-action  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data = pd.read_csv('../data/raw/comics_corpus.csv')\n",
    "comics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a77c2-c521-447b-ad4a-3db973b379b6",
   "metadata": {},
   "source": [
    "Podemos ver algumas frases duplicadas na feature `description`. Mas se explorarmos mais de perto, podemos ver que o que n√£o faz elas se tornarem 100% similares, s√£o as pontua√ß√µes. Portanto, vamos tratar as pontua√ß√µes para eliminar os exemplos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f25a5f4-ec2e-4758-b71c-95fd9107f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algum exemplo:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn‚Äôt Hel‚Äôs only ruler‚Äîand now she‚Äôs upset the cosmic balance. There will be a price to pay‚Ä¶and Karnilla intends to ensure the Valkyries pay it.\n",
      "\n",
      "Exemplo duplicado:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn?t Hel?s only ruler?and now she?s upset the cosmic balance. There will be a price to pay?and Karnilla intends to ensure the Valkyries pay it.\n"
     ]
    }
   ],
   "source": [
    "print(f'Algum exemplo:\\n{comics_data[\"description\"][1]}\\n')\n",
    "print(f'Exemplo duplicado:\\n{comics_data[\"description\"][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957d92e-6fad-41e5-b368-00a1dca68cbb",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### Pr√©-processamento da RNN\n",
    "Para um classificador de sentimentos, primeiro pr√©-processamos os dados brutos, depois, tokenizamos o nosso train set e extra√≠mos as features √∫teis para treinarmos o nosso modelo e fazermos nossas previs√µes.\n",
    "\n",
    "Plotando as stopwords em ing√™s e as pontua√ß√µes que ser√£o removidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61015b4e-cfc4-4c92-b2cb-0675e5125239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Pontua√ß√µes:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "print(f'Stopwords:\\n{stopwords_en}\\n\\nPontua√ß√µes:\\n{punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d082f99-cfc3-4a15-85b5-c1adc679eda3",
   "metadata": {},
   "source": [
    "<a name=\"2.1.1\"></a>\n",
    "#### Lowercasing, Stopwords, Stemming e Pontua√ß√µes\n",
    "Ap√≥s o pr√©-processamento inicial dos dados, acabamos apenas com as palavras que cont√™m todas as informa√ß√µes relevantes sobre o texto. Pr√©-processamentos iniciais antes da divis√£o e tokeniza√ß√£o:\n",
    "* `Lowercasing`: para reduzir nosso vocabul√°rio sem perder informa√ß√µes valiosas, teremos que colocar cada uma de nossas palavras em lowercase. Portanto, a palavra CHILDREN, Children e children, ser√£o tratadas como sendo exatamente a mesma palavra children.\n",
    "* `Caracteres especiais`: como s√≠mbolos matem√°ticos, s√≠mbolos monet√°rios, sinais de se√ß√£o e par√°grafo, sinais de marca√ß√£o inline e assim por diante. Geralmente √© seguro exclu√≠-los.\n",
    "* `Stopwords e pontua√ß√µes`: removemos todas as palavras que n√£o acrescentam significado significativo aos textos, tamb√©m conhecidas como stopwords e sinais de pontua√ß√£o (punctuation marks). Ap√≥s eliminadas, o significado geral da frase pode ser inferido sem nenhum esfor√ßo.\n",
    "* `Stemming`: √© simplesmente transformar qualquer palavra em sua base stem (raiz base), que podemos definir como o set de caracteres usados para construir a palavra e seus derivados. A palavra works por exemplo, seu stem √© `work`, porque, adicionando a letra \"s\", forma a palavra works, adicionando o sufixo \"e\", forma a palavra worke, e adicionando o sufixo \"ing\", forma a palavra working.\n",
    "    * Depois de executar a stemming no nosso corpus, as palavras works, worke e working, ser√£o reduzidas para a stem `work`. Portanto, nosso vocabul√°rio ser√° reduzido significativamente ao realizar esse processo para cada palavra do corpus.\n",
    "\n",
    "Pr√©-processando os dados, contando os exemplos inicialmente pr√©-processados e duplicados, e plotando os primeiros 5 primeiros exemplos do dataset inicialmente pr√©-processado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "671c01ed-8e03-4bd0-a7fe-2a160c8153b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de exemplos duplicados: 790\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "2  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "4  shark water x corp shock debut got fenc mend h...  non-action  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data.copy()\n",
    "comics_data_pre['description'] = comics_data_pre['description'].map(rnn_preprocess)\n",
    "print(f'N√∫mero de exemplos duplicados: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78006e44-8e62-4182-8d93-0b3b0affd4b4",
   "metadata": {},
   "source": [
    "Agora podemos remover os exemplos duplicados, porque est√£o inicialmente pr√©-processados e sem pontua√ß√µes.\n",
    "\n",
    "Plotando o n√∫mero de duplicatas ap√≥s a remova√ß√£o das duplicatas e plotando os primeiros 5 exemplos do dataset pr√©-processado e sem exemplos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78ea8c4d-3a67-43ad-a3f9-c4abe0e88f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de exemplos duplicados: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93645</td>\n",
       "      <td>Heroes Reborn: Weapon X &amp; Final Flight (2021) #1</td>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93052</td>\n",
       "      <td>Heroes Reborn (2021) #6</td>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             title  \\\n",
       "0  94799            Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339                    The Mighty Valkyries (2021) #3   \n",
       "3  93350                                  X-Corp (2021) #2   \n",
       "5  93645  Heroes Reborn: Weapon X & Final Flight (2021) #1   \n",
       "6  93052                           Heroes Reborn (2021) #6   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "5  best world without aveng squadron suprem prote...  non-action  \n",
       "6  eon fabl daughter utopia isl known power princ...  non-action  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data_pre.drop_duplicates('description')\n",
    "print(f'N√∫mero de exemplos duplicados: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff57126-3376-4b87-a05c-794d4b4003f6",
   "metadata": {},
   "source": [
    "Criando um dataset com apenas as features que ser√£o usadas no modelo, transformando o target label $y$ em bin√°rio e plotando os primeiros 5 exemplos do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cd0398d-b2da-4eb2-bd7a-03f1342cd2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  y\n",
       "0  shadow kirisaki mountain secret histori come l...  0\n",
       "1  children afterlif kraven hunter stalk jane fos...  1\n",
       "3  shark water x corp shock debut got fenc mend h...  0\n",
       "5  best world without aveng squadron suprem prote...  0\n",
       "6  eon fabl daughter utopia isl known power princ...  0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o novo dataset\n",
    "comics_corpus = comics_data_pre[['description', 'y']].copy()\n",
    "# Transformando o target label y em bin√°rio\n",
    "comics_corpus['y'] = comics_corpus['y'].map(lambda x: 1 if x == 'action' else 0)\n",
    "comics_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764098-e7e0-4a26-9a2e-7cc8b4835eba",
   "metadata": {},
   "source": [
    "Dividindo o dataset entre os subsets de treinamento, de valida√ß√£o e de teste. Usamos o split m√©todo `stratified sampling (amostragem estratificada)` para tentarmos manter a mesma propor√ß√£o dos labels na divis√£o entre cada subset, porque temos um leve desbalanceamento de classes e n√£o queremos que isso afete o nosso modelo.\n",
    "\n",
    "Dividindo o dataset e plotando a dimens√£o de cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93f68470-6e08-4394-b643-b80f8e183866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do subset de treinamento: (9682, 2)\n",
      "Shape do subset de valia√ß√£o: (3227, 2)\n",
      "Shape do subset de teste: (3228, 2)\n"
     ]
    }
   ],
   "source": [
    "# Dividindo entre treinamento e o subset da valida√ß√£o e teste\n",
    "split_train = StratifiedShuffleSplit(n_splits=1, test_size=.4, random_state=42)\n",
    "for train_index, subset_index in split_train.split(comics_corpus, comics_corpus['y']):\n",
    "    train_corpus, subset_corpus = comics_corpus.iloc[train_index, :].copy(), comics_corpus.iloc[subset_index, :].copy()\n",
    "\n",
    "# Dividindo entre valida√ß√£o e teste\n",
    "split_test = StratifiedShuffleSplit(n_splits=1, test_size=.5, random_state=42)\n",
    "for val_index, test_index in split_test.split(subset_corpus, subset_corpus['y']):\n",
    "    val_corpus, test_corpus = subset_corpus.iloc[val_index, :].copy(), subset_corpus.iloc[test_index, :].copy()\n",
    "\n",
    "print(f'Shape do subset de treinamento: {train_corpus.shape}\\nShape do subset de valia√ß√£o: {val_corpus.shape}\\nShape do subset de teste: {test_corpus.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9372fa-bc72-4079-8baf-84b413120439",
   "metadata": {},
   "source": [
    "Definindo as vari√°veis globais `VOCAB_SIZE` e `MAX_LEN` para tokenizar o training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18056891-e855-4c79-b132-fa93075d407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da maior sequ√™ncia limpa: 166\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "MAX_LEN = max([len(sentence.split()) for sentence in train_corpus['description']])\n",
    "print(f'Tamanho da maior sequ√™ncia limpa: {MAX_LEN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47db66-5cc9-4174-b0f2-af89b1ca2394",
   "metadata": {},
   "source": [
    "<a name=\"2.1.2\"></a>\n",
    "#### Tokeniza√ß√£o\n",
    "Nessa etapa, codificamos o nosso corpus do training set em sua representa√ß√£o vetorial, ou seja, primeiro precisamos criar um vocabul√°rio $V$ que nos permite codificar qualquer texto como um vetor de inteiros, por exemplo. Onde o nosso vocabul√°rio $V$ ser√° um vetor de palavras exclusivas do nosso vetor de textos, onde percorremos cada palavra de cada texto e salvamos no vocabul√°rio todas as novas palavras que aparecerem em nossa pesquisa. Ent√£o, mapeamos, ou seja, substitu√≠mos cada palavra encontrada no training set pelo seu √≠ndice no vocabul√°rio.\n",
    "| Token | √çndice |\n",
    "| :---: | :----: |\n",
    "| afterlif | 1 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| balanc | 615 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| cosmic | 621 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| work | VOCAB_SIZE |\n",
    "\n",
    "A RNN pode receber como input uma representa√ß√£o vetorial de frases tokenizadas e, ent√£o, aplicamos uma embedding layer que transformar√° nossa representa√ß√£o tokenizada de inteiros em valores que representam a sem√¢ntica dessa palavra num√©ricamente. Isso trata um dos problemas, a ordem das palavras, ordem alfab√©tica, neste exemplo, n√£o faz muito sentido do ponto de vista sem√¢ntico. Por exemplo, n√£o h√° raz√£o para que o 'cosmic' receba um n√∫mero maior do que o 'afterlif'.\n",
    "> Falarei mais sobre embeddings na cria√ß√£o do modelo.\n",
    "\n",
    "Essa RNN nos permitir√° prever sentimentos em frases complexas, que n√£o conseguir√≠amos classificar corretamente usando m√©todos mais simples, como Naive Bayes, porque eles perdem informa√ß√µes importantes.\n",
    "\n",
    "A nossa representa√ß√£o $X$ ser√° um vetor de n√∫meros inteiros, ou seja, o √≠ndice de cada token do nosso vocabul√°rio. Depois de obtermos todas as representa√ß√µes vetoriais das nossas frases, precisamos identificar o tamanho m√°ximo do vetor e preencher cada vetor com 0 para corresponder a essa tamanho, esse processo √© chamado de `padding` e garante que todos os nossos vetores tenham o mesmo tamanho, mesmo que nossas frases n√£o tenham.\n",
    "\n",
    "Para a cria√ß√£o do vocabul√°rio, definimos quais palavras pertencem ao vocabul√°rio. Isso significa que criamos uma lista com as palavras que vamos usar nas nossas representa√ß√µes. Uma forma de criar esse vocabul√°rio √© olhar para o nosso training set, e encontrar as `VOCAB_SIZE` palavras com mais ocorr√™ncia, por exemplo, ou utilizamos dicion√°rios j√° criados que nos digam as `VOCAB_SIZE` palavras mais comumente usadas na l√≠ngua da nossa tarefa.\n",
    "\n",
    "Em algumas tarefas como speech recognition, ou question aswering, encontraremos e geraremos somente palavras a partir de um set fixo de palavras, por exemplo, um chatbot s√≥ pode responder em sets limitados de perguntas. Essa lista fixa de palavras tamb√©m √© chamada de `closed vocabulary`. No entanto, usar um set fixo de palavras nem sempre √© suficiente para a tarefa. Muitas vezes, precisamos lidar com palavras que nunca vimos antes, o que resulta em um `open vocabulary`. **Open vocabulary** significa simplesmente que podemos encontrar palavras out of vocabulary (fora do vocabul√°rio), como o nome de uma nova cidade no training set.\n",
    "\n",
    "Se treinarmos uma rede neural em um corpus de textos com base nesse nosso vocabul√°rio, quando quisermos fazer infer√™ncia com o modelo treinado, precisaremos codificar o texto que desejamos inferir com o mesmo vocabul√°rio. Caso contr√°rio, n√£o far√° sentido, porque as palavras seriam mapeadas para n√∫meros diferentes, tokens diferentes. Qualquer palavra no corpus de treino que n√£o esteja no vocabul√°rio ser√° substitu√≠da por `<UNK>`. As unknown words tamb√©m s√£o chamadas de `Out of vocabulary (OOV)`. Uma forma de lidar com palavras OOV √© model√°-las com uma palavra especial, **\\<UNK>**. Para fazermos isso, substitu√≠mos todas as palavras OOV por \\<UNK>, o special token, `<UNK>`. A propor√ß√£o de unknown words no test set √© chamada de `OOV rate`.\n",
    "\n",
    "No final, aplicamos o `padding`, adicionamos 0 ao final de cada sequence e fazemos com que todas tenham o mesmo comprimento. Isso √© chamado de `post-padding (p√≥s-padding)`, porque os tokens de padding est√£o no final das sequ√™ncias.\n",
    "\n",
    "O tensorflow e o Keras nos oferecem v√°rias maneiras de tokenizar palavras. Uma delas √© a layer que estou usando nesse sistema, [`tensorflow.keras.layers.TextVectorization()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization).\n",
    "* `TextVectorization()`: gerar√° o vocabul√°rio e criar√° vetores a partir das frases. Ela retira pontua√ß√µes, ou seja, ela ir√° gerenciar os tokens, transformando as frases em uma lista de inteiros (os √≠ndices de cada token no vocabul√°rio) e etc. \n",
    "* `adapt()`: m√©todo da layer TextVectorization(), que pega os dados e gera um vocabul√°rio a partir das palavras encontradas nessas frases.\n",
    "\n",
    "Treinando o tokenizador no training set com o `VOCAB_SIZE` e `MAX_LEN` definidos anteriormente e projetando o tamanho do vocabul√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4cd23d50-369d-488b-9376-c900c09a591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabul√°rio: 1000\n"
     ]
    }
   ],
   "source": [
    "sentence_vec = rnn_tokenizer(train_corpus['description'], max_tokens=VOCAB_SIZE, max_len=MAX_LEN)\n",
    "print(f'Tamanho do vocabul√°rio: {sentence_vec.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7e43b-5e54-43c3-831d-756992450d70",
   "metadata": {},
   "source": [
    "Aplicando o tokenizador treinado em cada subset e plotando as suas dimens√µes e o primeiro exemplo tokenizado do training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "671f84aa-426e-4de4-9a4b-c446f81ac9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimens√£o das sequ√™ncias de treino tokenizadas e com padding: (9682, 166)\n",
      "\n",
      "Primeira sequ√™ncia de treino com padding:\n",
      "[202 101  80  45 332 101  80 804 323 767   1   1 815 649 499 795 303  92\n",
      " 409 624   3   1   1   1   1 413   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "\n",
      "Dimens√£o das sequ√™ncias de valida√ß√£o tokenizadas e com padding: (3227, 166)\n",
      "Dimens√£o das sequ√™ncias de teste tokenizadas e com padding: (3228, 166)\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = sentence_vec(train_corpus['description'])\n",
    "val_tokenized = sentence_vec(val_corpus['description'])\n",
    "test_tokenized = sentence_vec(test_corpus['description'])\n",
    "print(f'Dimens√£o das sequ√™ncias de treino tokenizadas e com padding: {train_tokenized.shape}\\n\\nPrimeira sequ√™ncia de treino com padding:\\n{train_tokenized[0]}\\n')\n",
    "print(f'Dimens√£o das sequ√™ncias de valida√ß√£o tokenizadas e com padding: {val_tokenized.shape}')\n",
    "print(f'Dimens√£o das sequ√™ncias de teste tokenizadas e com padding: {test_tokenized.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e42e-a0f3-4770-9218-ed41877a2d4f",
   "metadata": {},
   "source": [
    "Carregando o modelo de tokeniza√ß√£o treinado no diret√≥rio `../models/` para usarmos posteriormente. Salvamos os hiperpar√¢metros que foram utilizados no treinamento e o vocabul√°rio gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e20a20-0271-4fa3-9dbf-78da1a2d6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    {'config': sentence_vec.get_config(), 'vocabulary': sentence_vec.get_vocabulary()},\n",
    "    open('../models/vectorizer.pkl', 'wb')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32031-b879-4358-b2a8-22a7ab39427e",
   "metadata": {},
   "source": [
    "Transformando os labels $y$ em um vetor de coluna, concatenando cada corpus tokenizado com os labels $y$ correspondentes e plotando a dimens√£o de cada subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0738867-7ac3-4312-aff8-26a8eefea2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimens√£o do subset de treino pr√©-processado: (9682, 167)\n",
      "Dimens√£o do subset de valida√ß√£o pr√©-processado: (3227, 167)\n",
      "Dimens√£o do subset de teste pr√©-processado: (3228, 167)\n"
     ]
    }
   ],
   "source": [
    "# Transformando os labels y em um vetor de coluna\n",
    "labels_train = train_corpus[['y']].copy()\n",
    "labels_val = val_corpus[['y']].copy()\n",
    "labels_test = test_corpus[['y']].copy()\n",
    "\n",
    "# Concatenando o corpus de cada subset e os labels correspondentes\n",
    "train_tokens = np.concatenate([train_tokenized, labels_train], axis=1)\n",
    "val_tokens = np.concatenate([val_tokenized, labels_val], axis=1)\n",
    "test_tokens = np.concatenate([test_tokenized, labels_test], axis=1)\n",
    "print(f'Dimens√£o do subset de treino pr√©-processado: {train_tokens.shape}')\n",
    "print(f'Dimens√£o do subset de valida√ß√£o pr√©-processado: {val_tokens.shape}')\n",
    "print(f'Dimens√£o do subset de teste pr√©-processado: {test_tokens.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af5b1-21f5-411a-bfbf-77385bdf293f",
   "metadata": {},
   "source": [
    "Carregando cada dataset pr√©-processado no diret√≥rio `../data/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5444967e-b6d0-4877-853c-10d2e41d6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando no disco o dataset com pr√©-processamento inicial\n",
    "comics_corpus.to_csv('../data/preprocessed/comics_corpus.csv', index=False)\n",
    "# Carregando no disco os datasets tokenizados\n",
    "np.save('../data/preprocessed/train_tokens.npy', train_tokens)\n",
    "np.save('../data/preprocessed/validation_tokens.npy', val_tokens)\n",
    "np.save('../data/preprocessed/test_tokens.npy', test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60216a38-23ab-4aad-bc3f-99f5195e81ad",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### Pr√©-processamento do Transformers\n",
    "Para o pr√©-processamento do transformers, vamos utilizar um tokenizador pr√©-treinado do checkpoint do `DistilBERT` para aplicar a tokeniza√ß√£o e o padding.\n",
    "\n",
    "DistilBERT √© um modelo Transformers pequeno, r√°pido, barato e leve treinado pela destila√ß√£o do modelo base BERT (Bidirectional Encoder Representation from Transformers). Ele tem 40% menos par√¢metros que o bert-base-uncased, roda 60% mais r√°pido e preserva mais de 95% do desempenho do Bert conforme medido no benchmark GLUE (General Language Understanding Evaluation).\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (ü§ó) √© o melhor recurso para transformers pr√©-treinados. Suas bibliotecas open-source simplificam o download, o fine-tuning e o uso de modelos de transformers como DeepSeek, BERT, Llama, T5, Qwen, GPT-2 e muito mais. E a melhor parte, voc√™ pode us√°-los junto com TensorFlow, PyTorch ou Flax. Neste sistema, utilizo transformers ü§ó para usar o modelo `DistilBERT` para classifica√ß√£o de sentimento. Para a etapa do pr√©-processamento, usamos o tokenizador do DistilBERT `distilbert-base-uncased-finetuned-sst-2-english` pr√©-treinado, para isso inicializamos a classe DistilBertTokenizer e definidos o modelo pr√©-treinado desejado.\n",
    "> No fine-tuning do modelo, no notebook `05_transformers_finetuning.ipynb`, falarei com mais detalhes sobre o transfer learning e o fine-tuning.\n",
    "\n",
    "Tokenizando, aplicando o padding no corpus e retornando o corpus tokenizado como tensores pytorch utilizando o tokenizer `DistilBERT` pr√©-treinado. Definindo o tokenizer e plotando as representa√ß√µes vetoriais do corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4600970c-28f4-4ea8-9ffb-a28b511145cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1999,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1037,  3181,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  2028,  ...,     0,     0,     0],\n",
       "        [  101, 16228,  2023,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o modelo pr√©-treinado\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "# Definindo o tokenizer\n",
    "comics_transformers = tokenizer(\n",
    "    comics_data['description'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "# Acessando as representa√ß√µes vetoriais do corpus\n",
    "transformers_tokens = comics_transformers['input_ids']\n",
    "transformers_attention = comics_transformers['attention_mask']\n",
    "transformers_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7d949-f92d-47ea-b8f2-b004af97ba74",
   "metadata": {},
   "source": [
    "Selecionando os labels do dataset bruto e dividindo o tensor tokenizado entre os subsets de treinamento, de valida√ß√£o e de teste. Usamos o m√©todo de split `stratified sampling (amostragem estratificada)` para tentarmos manter a mesma propor√ß√£o dos labels na divis√£o entre cada subset.\n",
    "\n",
    "Dividindo o dataset e plotando o tamanho de cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97f49488-f556-47c9-bcb9-33ac7988ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do subset de treino: 10156\n",
      "Tamanho do subset de valida√ß√£o: 3385\n",
      "Tamanho do subset de teste: 3386\n"
     ]
    }
   ],
   "source": [
    "# Selecionando os labels do dataset bruto\n",
    "labels = (comics_data['y']\n",
    "          .map(lambda x: 1 if x == 'action' else 0)\n",
    "          .to_numpy()\n",
    "          .reshape(-1, 1))\n",
    "\n",
    "# Dividindo entre treinamento e o subset da valida√ß√£o e teste\n",
    "train_idx, subset_idx = next(split_train.split(transformers_tokens, labels))\n",
    "# Dividindo entre valida√ß√£o e teste\n",
    "val_idx, test_idx = next(split_test.split(transformers_tokens[subset_idx], labels[subset_idx]))\n",
    "print(f'Tamanho do subset de treino: {len(train_idx)}\\nTamanho do subset de valida√ß√£o: {len(val_idx)}\\nTamanho do subset de teste: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba41bb9-9768-4121-8ebc-e982a80102b8",
   "metadata": {},
   "source": [
    "Transformando os subsets de tensores pytorch tokenizados divididos pela `stratified sampling split` para o tipo `Dataset` em formato de dicion√°rio, para executarmos o fine-tuning do transformers e plotando cada dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b657d0b8-5c4c-413e-aa4c-2aa37c8e134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treino: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Dataset de valida√ß√£o: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Dataset de teste: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, train_idx)\n",
    "val_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, val_idx)\n",
    "test_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, test_idx)\n",
    "print(f'Dataset de treino: {train_dataset}\\nDataset de valida√ß√£o: {val_dataset}\\nDataset de teste: {test_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7634f8-e5e5-4a58-bbd0-289af0787473",
   "metadata": {},
   "source": [
    "Carregando cada dataset pr√©-processado e seus metadados dentro de seu diret√≥rio espec√≠fico dentro do diret√≥rio `../data/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d767741-02a8-493a-a30d-f1ebec36b2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3278c9f7c64bb88eded1df5b529eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf2b8fe7fed4753b4811cd6fb13f290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7784715774b4fde89c017ddbf2206fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('../data/preprocessed/train_dataset')\n",
    "val_dataset.save_to_disk('../data/preprocessed/validation_dataset')\n",
    "test_dataset.save_to_disk('../data/preprocessed/test_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
