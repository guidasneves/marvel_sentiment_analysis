{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f339ad48-c1bf-4e5c-83c0-e73d2fc7a296",
   "metadata": {},
   "source": [
    "# Etapa do Fine-tuning do Transformers\n",
    "Etapa do fine-tuning no modelo DistilBERT pré-treinado nos nossos dados, para depois, salvarmos o modelo e seus respectivos pesos treinados.\n",
    "\n",
    "Nesta etapa realizamos:\n",
    "* Definição do modelo;\n",
    "* Fine-tuning do modelo;\n",
    "* Avaliação do modelo;\n",
    "* Salvamento do modelo e dos pesos treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6776b-a00c-4755-8110-bfdf4188cc94",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Pacotes](#1)\n",
    "* [Carregando os Dados](#2)\n",
    "* [Transformers](#3)\n",
    "* [Transfer Learning](#4)\n",
    "    * [Fine-tuning](#4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf3166-e70e-4490-a37b-1c681cf8966d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Pacotes\n",
    "Pacotes que foram utilizados no sistema:\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): fornece APIs e ferramentas para baixar e treinar facilmente modelos pré-treinados de última geração;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): é uma biblioteca para acessar e compartilhar facilmente datasets para tarefas de áudio, visão computacional e processamento de linguagem natural (NLP);\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): biblioteca open-source de machine learning;\n",
    "* [src](../src/): pacote com todos os códigos de todas as funções utilitárias criadas para esse sistema. Localizado dentro do diretório `../src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a4f5ea5-7a61-4b66-9a59-c6b7ab71e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Obtendo a versão absoluta normalizada do path raíz do projeto\n",
    "    os.path.join( # Concatenando os paths\n",
    "        os.getcwd(), # Obtendo o path do diretório dos notebooks\n",
    "        os.pardir # Obtendo a string constante usada pelo OS para fazer referência ao diretório pai\n",
    "    )\n",
    ")\n",
    "# Adicionando o path à lista de strings que especifica o path de pesquisa para os módulos\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.transformers_finetuning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acb6b9-6ba5-49e1-a00d-b90a63beeb67",
   "metadata": {},
   "source": [
    "> **Nota**: os códigos para as funções utilitárias utilizadas nesse sistema estão no script `transformers_finetuning.py` dentro do diretório `../src/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead2267-48de-4889-aac5-dea58282b080",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Carregando os Dados\n",
    "Vamos ler cada um dos subsets dentro de seus respectivos diretórios dentro do diretório `../data/preprocessed/` e plotar cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e38bf67-f145-47b7-bfa7-5149743df283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do train set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Shape do validation set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Shape do test set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset.load_from_disk('../data/preprocessed/train_dataset')\n",
    "val_set = Dataset.load_from_disk('../data/preprocessed/validation_dataset')\n",
    "test_set = Dataset.load_from_disk('../data/preprocessed/test_dataset')\n",
    "print(f'Shape do train set: {train_set}\\nShape do validation set: {val_set}\\nShape do test set: {test_set}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37646c-d5a8-4829-a7d7-76080f57e6d3",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## Transformers\n",
    "<img align='center' src='../figures/transformers.png' style='width:400px;'>\n",
    "Transformer é um modelo puramente baseado em attention que foi desenvolvido pelo Google para solucionar alguns problemas com RNNs, como o de ser difícil de explorar totalmente as vantagens da computação paralela, devido aos seus problemas decorrentes de sua estrutura sequencial. Em um RNN seq2seq, precisamos passar por cada palavra de nosso input, de forma sequencial pelo encoder, e é feito de uma forma sequencial similar pelo decoder, sem computação paralela. Por esse motivo, não há muito espaço para cálculos paralelos. Quanto mais palavras temos na sequência de input, mais tempo será necessário para processá-la.\n",
    "\n",
    "Com sequências grandes, ou seja, com muitos $T$ steps sequenciais, as informações tendem a se perder na rede (loss of information) e problemas de vanishing gradients surgem relacionados ao comprimento de nossas input sequences. LSTMs e GRUs ajudam um pouco com esses problemas, mas mesmo essas arquiteturas param de funcionar bem quando tentam processar sequências muito longas devido ao `information bottleneck`.\n",
    "* `Loss of information`: é mais difícil saber se o sujeito é singular ou plural à medida que nos afastamos do sujeito.\n",
    "* `Vanishing gradients`: quando calculamos o backprop, os gradients podem se tornar muito pequenos e, como resultado, o modelo não aprenderá nada.\n",
    "\n",
    "Os transformers se baseiam em attention e não exigem nenhum cálculo sequencial por layer, sendo necessário apenas um único step. Além disso, os steps de gradient que precisam ser realizados do último output para o primeiro output em um transformers são apenas 1. Para RNNs, o número de steps aumenta com sequências mais longas. Por fim, os transformers não sofrem com problemas de vanishing gradients relacionados ao comprimento das sequências.\n",
    "\n",
    "Transformers não usam RNNs, attention é tudo o que precisamos, e apenas algumas transformações lineares e não lineares geralmente são incluídas. O modelo transformers foi introduzido em 2017 por pesquisadores do Google, e desde então, a arquitetura do transformer se tornou padrão para LLMs. Os transformers revolucionaram o campo de NLP.\n",
    "\n",
    "O modelo transformers usa `scaled dot-product attention`. A primeira forma da attention é muito eficiente em termos de computação e memória, porque consiste apenas em operações de multiplicações de matrizes. Esse mecanismo é o núcleo do modelo e permite que o transformers cresça e se torne mais complexo, sendo mais rápido e usando menos memória do que outras arquiteturas de modelos comparáveis.\n",
    "\n",
    "No modelo transformers, usaremos a `Multi-Head Attention layer`, essa layer é executada em paralelo e tem vários mecanismos scaled dot-product attention $h$ e várias transformações lineares dos input queries $Q$, keys $K$ e values $V$. Nessa layer, as transformações lineares são parâmetros treináveis.\n",
    "$$\\text{ Attention}(Q, K, V) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V$$\n",
    "<img align='center' src='../figures/attention.png' style='width:600px;'>\n",
    "\n",
    "Os transformers `encoders` começam com um módulo multi-head attention que executa a `self-attention` na input sequence. Isso é seguido por uma residual connection e normalização, em seguida, uma feed forward layer e outra residual connection e normalização. A encoder layer é repetida $N_x$ vezes.\n",
    "* Self-attention: cada palavra no input corresponde a cada outra palavra no input sequence.\n",
    "* Graças à self-attention layer, o encoder fornecerá uma representação contextual de cada um de nossos inputs.\n",
    "\n",
    "\n",
    "O `decoder` é construído de forma similar ao encoder, com módulos multi-head attention, residual connections e normalização. O primeiro módulo de attention é mascarado (`Masked Self-Attention`) de forma que cada posição atenda apenas às posições anteriores, ele bloqueia o fluxo de informações para a esquerda. O segundo módulo de attention (`Encoder-Decoder Attention`) pega o output do encoder e permite que o decoder atenda a todos os itens. Toda essa decoder layer também é repetida várias $N_x$ vezes, uma após a outra.\n",
    "$$\\text{ Masked Self-Attention } = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + M \\right) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + \\text{ mask matrix } \\begin{pmatrix} 0 & -\\infty & -\\infty \\\\ 0 & 0 & -\\infty \\\\ 0 & 0 & 0 \\end{pmatrix} \\right)$$\n",
    "\n",
    "Os transformers também incorporam um `positional encoding stage` ($PE$), que codifica a posição de cada input na sequência, ou seja, as informações sequenciais. Isso é necessário porque os transformers não usam RNNs, mas a ordem das palavras é relevante para qualquer idioma. A positional encoding é treinável, assim como as word embeddings.\n",
    "$$\\begin{align*}\n",
    "& \\text{PE}_{(\\text{pos, }2i)} = \\text{sin}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right) \\\\\n",
    "& \\text{PE}_{(\\text{pos, }2i + 1)} = \\text{cos}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Primeiro, é calculado o embedding sobre o input e as positional encodings são aplicadas. Então, isso vai para o encoder que consiste em várias layers de módulos de Multi-Head Attention, em seguida, o decoder recebe a sequência de output deslocada um step para a direita e os outputs do encoder. O output do decoder é transformado em probabilidades de outputs usando uma layer linear com uma ativação softmax. Essa arquitetura é fácil de paralelizar em comparação com os modelos RNNs e pode ser treinada com muito mais eficiência em várias GPUs. Ela também pode ser escalada para aprender várias tarefas em datasets cada vez maiores. Transformers são uma ótima alternativa aos RNNs e ajudam a superar esses problemas em NLP e em muitos campos que processam dados sequenciais.\n",
    "\n",
    "Faremos o fine-tuning no modelo DistilBERT, que é um modelo Transformer pequeno, rápido, barato e leve, treinado pela destilação do modelo base BERT. Ele tem 40% menos parâmetros que o bert-base-uncased, roda 60% mais rápido e preserva mais de 95% do desempenho do BERT conforme medido no benchmark GLUE (General Language Understanding Evaluation).\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (🤗) é o melhor recurso para transformers pré-treinados. Suas bibliotecas de código aberto simplificam o download, o fine-tuning e o uso de modelos de transformers como DeepSeek, BERT, Llama, T5, Qwen, GPT-2 e muito mais. E a melhor parte, podemos usá-los junto com TensorFlow, PyTorch ou Flax. Neste notebook, utilizo transformers 🤗 para usar o modelo `DistilBERT` para classificação de sentimento. Para a etapa do pré-processamento, usamos o tokenizador (no notebook `03_preprocessing.ipynb`), e no fine-tuning do checkpoint do DistilBERT `distilbert-base-uncased-finetuned-sst-2-english` pré-treinado no código abaixo. Para isso inicializamos a classe DistilBertForSequenceClassification e definidos o modelo pré-treinado desejado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a315b-c133-4fac-a7f8-f175550ad4e2",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## Transfer Learning\n",
    "Para o modelo transformers, utilizamos a técnica que transfer learning. Ela é uma das ideias mais poderosas do deep learning, que às vezes, podemos pegar o conhecimento que a rede neural aprendeu em uma tarefa e aplicar esse conhecimento em uma tarefa separada. Há 3 vantagens principais no transfer learning são:\n",
    "* Reduz o tempo de treinamento.\n",
    "* Melhora as previsões.\n",
    "* Nos permite usar datasets menores.\n",
    "\n",
    "Se estamos criando um modelo, em vez de treinarmos os pesos do 0, a partir de uma inicialização aleatória, geralmente progredimos muito mais rápido baixando pesos que outra pessoa já treinou por dias/semanas/meses na arquitetura da rede neural, as usamos como pré-treinamento, e as transferimos para uma nova tarefa semelhante na qual possamos estar interessados. Isso significa que muitas vezes podemos baixar pesos de algoritmos open-source, que outras pessoas levaram semanas ou meses para descobrir, e então, usamos isso como uma inicialização muito boa para nossa rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9148a-39fe-4150-bc54-7976a67f4e45",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### Fine-tuning\n",
    "Com o modelo pré-treinado, aplicamos o fine-tuning. Então, pegamos os pesos de um modelo pré-treinado existente, utilizando transfer learning e, em seguida, ajustamos um pouco para garantir que funcionem na tarefa específica em que estamos trabalhando. Digamos que pré-treinamos um modelos que prevê a avaliação de filmes, e agora vamos criar um modelo para avaliar cursos. Uma maneira de fazer isso é, bloqueando todos os pesos que já temos pré-treinados e, em seguida, adicionamos uma nova output layer, ou talvez, uma nova feed forward layer e uma output layer que serão treinadas, enquanto mantemos o restante bloqueado e, em seguida, treinamos apenas a nossa nova rede, as novas layers que acabamos de adicionar. Podemos descongelar lentamente as layers, uma de cada vez.\n",
    "\n",
    "Muitas das features de baixo nível que o modelo pré-treinado aprendeu a partir de um corpus muito grande, como a estrutura do texto, a natureza do texto, isso pode ajudar nosso algoritmo a se sair melhor na tarefa de classificação de sentimentos e mais rápido ou com menos dados, porque talvez o modelo tenha aprendido o suficiente como são as estruturas de textos diferentes e parte desse conhecimento será útil. Após excluirmos a output layer de um modelo pré-treinado, não precisamos necessariamente criar apenas a output layer, mas podemos criar várias novas layers.\n",
    "\n",
    "Precisamos remover a output layer do modelo pré-treinado e adicionar a nossa, porque a rede neural pode ter uma softmax output layer que gera um dos 1000 labels possíveis. Então removemos essa output layer e criamos a nossa própria output layer, nesse caso uma ativação sigmoid.\n",
    "\n",
    "* Com um training set pequeno, pensamos no restante das layers como `congeladas`, então congelamos os parâmetros dessas layers, e treinamos apenas os parâmetros associados à nossa output layer. Dessa forma obteremos um desempenho muito bom, mesmo com um training set pequeno.\n",
    "\n",
    "* Com um training set maior, nós podemos congelar menos layers e então treinar as layers que não foram congeladas e a nossa nova output layer. Podemos usar as layers que não estão congeladas como inicialização e usar o gradient descent a partir delas, ou também podemos eliminar essas layers que não estão congeladas e usamos nossas próprias hidden layers novas e nossa própria output layer. Qualquer um desses métodos pode valer a pena tentar.\n",
    "\n",
    "* Com um training set muito maior usamos essa rede neural pré-treinada e os seus pesos como inicialização e treinamos toda a rede neural, apenas alterando a output layer, com labels que nos importamos.\n",
    "\n",
    "Definindo o checkpoint do modelo pré-treinado que faremos o fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b6dd67-2714-4583-81d0-f8db9e01fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e5df1-9a50-43d1-b08b-da5943921f30",
   "metadata": {},
   "source": [
    "Definindo os hiperparâmetros com o objeto `TrainingArguments`, usando o objeto `Trainer` do Hugging Face para realizar o fine-tuning do modelo.\n",
    "* O modelo pré-treinado com o fine-tuning já está sendo salvo no diretório `../models/`, definido na definição dos hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1585ffdb-779b-420d-81f3-65fbe73251af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2540' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2540/2540 7:12:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.592717</td>\n",
       "      <td>0.684184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.697679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.585440</td>\n",
       "      <td>0.712016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.488176</td>\n",
       "      <td>0.756529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.502221</td>\n",
       "      <td>0.761378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.474515</td>\n",
       "      <td>0.775508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.474097</td>\n",
       "      <td>0.764560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.448737</td>\n",
       "      <td>0.778758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.487454</td>\n",
       "      <td>0.767549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.445698</td>\n",
       "      <td>0.818367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.394227</td>\n",
       "      <td>0.821744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.485700</td>\n",
       "      <td>0.402096</td>\n",
       "      <td>0.823441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.470619</td>\n",
       "      <td>0.817294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.383477</td>\n",
       "      <td>0.831915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.392066</td>\n",
       "      <td>0.833519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.384125</td>\n",
       "      <td>0.846788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.389733</td>\n",
       "      <td>0.843293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.395148</td>\n",
       "      <td>0.846830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.388265</td>\n",
       "      <td>0.837942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.351318</td>\n",
       "      <td>0.849480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.309669</td>\n",
       "      <td>0.867014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.387884</td>\n",
       "      <td>0.869130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.325386</td>\n",
       "      <td>0.870885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.344718</td>\n",
       "      <td>0.875784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2540, training_loss=0.4370420598608302, metrics={'train_runtime': 25970.5016, 'train_samples_per_second': 0.782, 'train_steps_per_second': 0.098, 'total_flos': 2690677801500672.0, 'train_loss': 0.4370420598608302, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hiperparâmetros do fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/transformers_results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=20,\n",
    "    weight_decay=1e-1,\n",
    "    eval_strategy='steps',\n",
    "    lr_scheduler_type='reduce_lr_on_plateau',\n",
    "    logging_steps=100\n",
    ")\n",
    "# Objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=f1_metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff61dd2-b950-461e-b805-d43abf11a9ca",
   "metadata": {},
   "source": [
    "Avaliando o desempenho do modelo com fine-tuning no train e validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38acb7b3-8745-410e-8db4-7f05352bb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação do train set: 0.9178\n",
      "Avaliação do validation set: 0.8523\n"
     ]
    }
   ],
   "source": [
    "print(f'Avaliação do train set: {trainer.evaluate(train_set)[\"eval_f1_score\"]:.4f}\\nAvaliação do validation set: {trainer.evaluate(val_set)[\"eval_f1_score\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bfb82-0c3e-430d-b4e4-5debe24f6ca5",
   "metadata": {},
   "source": [
    "Avaliando o desempenho do modelo final com fine-tuning no test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14f3b133-8ef9-46c0-9a67-517822821220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação do test set: 0.8518\n"
     ]
    }
   ],
   "source": [
    "print(f'Avaliação do test set: {trainer.evaluate(test_set)[\"eval_f1_score\"]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
