{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debe9ec0-89c7-46ab-9203-b210db8004cc",
   "metadata": {},
   "source": [
    "# Preprocessing Step (Etapa de Pré-Processamento)\n",
    "**[EN-US]**\n",
    "\n",
    "Data pre-processing stage and then we feed this data to the model.\n",
    "\n",
    "In this step we perform:\n",
    "* Transformation of all words to lowercase;\n",
    "* Removal of punctuations;\n",
    "* Removal of stopwords;\n",
    "* We apply stemming;\n",
    "* Splitting between training, validation and testing;\n",
    "* Tokenization;\n",
    "* Padding.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Etapa do pré-processamento dos dados para depois, alimentarmos esses dados ao modelo.\n",
    "\n",
    "Nesta etapa realizamos:\n",
    "* Transformação de todas as palavras para minúsculas;\n",
    "* Remoção de pontuações;\n",
    "* Remoção de stopwords;\n",
    "* Aplicamos o stemming;\n",
    "* Divisão entre treino, validação e teste;\n",
    "* Tokenização;\n",
    "* Padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa711489-0961-4df2-af39-63583899af35",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Packages](#1)\n",
    "* [Preprocessing](#2)\n",
    "    * [RNN Preprocessing](#2.1)\n",
    "        * [Lowercasing, Stopwords, Stemming and Punctuations](#2.1.1)\n",
    "        * [Tokenization](#2.1.2)\n",
    "    * [Transformer Preprocessing](#2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec45f-ccfd-46c9-913b-601cf152c125",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Packages (Pacotes)\n",
    "**[EN-US]**\n",
    "\n",
    "Packages used in the system.\n",
    "* [pandas](https://pandas.pydata.org/): is the main package for data manipulation;\n",
    "* [numpy](www.numpy.org): is the main package for scientific computing;\n",
    "* [re](https://docs.python.org/3/library/re.html): provides regular expression matching operations similar to those found in Perl;\n",
    "* [string](https://docs.python.org/pt-br/3.13/library/string.html): for common string operations;\n",
    "* [nltk](https://www.nltk.org/): NLTK is a leading platform for building Python programs to work with human language data;\n",
    "* [tensorflow](https://www.tensorflow.org/): framework that makes it easy to create ML models that can run in any environment;\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): open source machine learning library;\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html): implements binary protocols for serializing and de-serializing a Python object structure;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): provides APIs and tools to easily download and train state-of-the-art pretrained models;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks;\n",
    "* [os](https://docs.python.org/3/library/os.html): built-in module, provides a portable way of using operating system dependent functionality;\n",
    "* [sys](https://docs.python.org/3/library/sys.html): provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter;\n",
    "* [src](../src/): package with all the codes for all utility functions created for this system. Located inside the `../src/` directory.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Pacotes utilizados no sistema.\n",
    "* [pandas](https://pandas.pydata.org/): é o principal pacote para manipulação de dados;\n",
    "* [numpy](www.numpy.org): é o principal pacote para computação científica;\n",
    "* [re](https://docs.python.org/3/library/re.html): fornece operações de correspondência de expressões regulares semelhantes às encontradas em Perl;\n",
    "* [string](https://docs.python.org/pt-br/3.13/library/string.html): para operações comuns de strings;\n",
    "* [nltk](https://www.nltk.org/): NLTK é uma plataforma líder para a construção de programas Python para trabalhar com dados de linguagem humana;\n",
    "* [tensorflow](https://www.tensorflow.org/): framework que facilita a criação de modelos de machine learning que podem ser executados em qualquer ambiente;\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): biblioteca open-source de machine learning;\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html): implementa protocolos binários para serializar e desserializar uma estrutura de objeto Python;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): fornece APIs e ferramentas para baixar e treinar facilmente modelos pré-treinados de última geração;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): é uma biblioteca para acessar e compartilhar facilmente datasets para tarefas de áudio, visão computacional e processamento de linguagem natural (NLP);\n",
    "* [os](https://docs.python.org/3/library/os.html): módulo integrado, fornece uma maneira portátil de usar funcionalidades dependentes do sistema operacional;\n",
    "* [sys](https://docs.python.org/3/library/sys.html): fornece acesso a algumas variáveis usadas ou mantidas pelo interpretador e a funções que interagem fortemente com o interpretador;\n",
    "* [src](../src/): pacote com todos os códigos de todas as funções utilitárias criadas para esse sistema. Localizado dentro do diretório `../src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375d1727-909a-4086-ba17-f377c05c39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Getting Obtaining the absolute normalized version of the project root path (Obtendo a versão absoluta normalizada do path raíz do projeto)\n",
    "    os.path.join( # Concatenating the paths (Concatenando os paths)\n",
    "        os.getcwd(), # # Getting the path of the notebooks directory (Obtendo o path do diretório dos notebooks)\n",
    "        os.pardir # Gettin the constant string used by the OS to refer to the parent directory (Obtendo a string constante usada pelo OS para fazer referência ao diretório pai)\n",
    "    )\n",
    ")\n",
    "# Adding path to the list of strings that specify the search path for modules\n",
    "# Adicionando o path à lista de strings que especifica o path de pesquisa para os módulos\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb33b25-3901-447b-9a3d-3aaaf12f60ad",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "> **Note**: the codes for the utility functions used in this system are in the `preprocessing.py` script within the `../src/` directory.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "> **Nota**: os códigos para as funções utilitárias utilizadas nesse sistema estão no script `preprocessing.py` dentro do diretório `../src/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e78a5-0cc8-4cd6-b8f2-4373e4035379",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Preprocessing (Pré-processamento)\n",
    "**[EN-US]**\n",
    "\n",
    "We will create 2 models for this project. Therefore, we will do 2 different preprocessings, one for each model:\n",
    "1. The first will be using an RNN with bidirectional LSTMs created and trained from scratch.\n",
    "2. The second model will be the application of fine-tuning to a model with pre-trained transformers architecture.\n",
    "\n",
    "Reading the dataset that will be pre-processed and plotting your first 5 examples.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Vamos criar 2 modelos para esse projeto. Portanto, faremos 2 pré-processamentos diferentes, um para cada modelo:\n",
    "1. O primeiro será utilizando uma RNN com LSTMs bidirecionais criadas e treinada do zero.\n",
    "2. O segundo modelo será a aplicação de fine-tuning em um modelo com arquitetura transformers pré-treinado.\n",
    "\n",
    "Lendo o dataset que será pré-processado e projetando os seus primeiros 5 exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e195ab81-50b4-4e6a-8eb9-0baaa1431269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>CHILDREN OF THE AFTERLIFE! While Kraven the Hu...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP’s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>A SHARK IN THE WATER! After X-CORP?s shocking ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  IN THE SHADOW OF KIRISAKI MOUNTAIN?A SECRET HI...  non-action  \n",
       "1  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "2  CHILDREN OF THE AFTERLIFE! While Kraven the Hu...      action  \n",
       "3  A SHARK IN THE WATER! After X-CORP’s shocking ...  non-action  \n",
       "4  A SHARK IN THE WATER! After X-CORP?s shocking ...  non-action  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data = pd.read_csv('../data/raw/comics_corpus.csv')\n",
    "comics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a77c2-c521-447b-ad4a-3db973b379b6",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "We can see some duplicate phrases in the `description` feature. But if we explore more closely, we can see that what doesn't make them 100% similar are the scores. Therefore, let's treat the scores to eliminate duplicate examples.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Podemos ver algumas frases duplicadas na feature `description`. Mas se explorarmos mais de perto, podemos ver que o que não faz elas se tornarem 100% similares, são as pontuações. Portanto, vamos tratar as pontuações para eliminar os exemplos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f25a5f4-ec2e-4758-b71c-95fd9107f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some example:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn’t Hel’s only ruler—and now she’s upset the cosmic balance. There will be a price to pay…and Karnilla intends to ensure the Valkyries pay it.\n",
      "\n",
      "Duplicated example:\n",
      "CHILDREN OF THE AFTERLIFE! While Kraven the Hunter stalks Jane Foster on Midgard and the newest Valkyrie fights for her soul on Perdita, Karnilla, the queen of Hel, works a miracle in the land of the dead! But Karnilla isn?t Hel?s only ruler?and now she?s upset the cosmic balance. There will be a price to pay?and Karnilla intends to ensure the Valkyries pay it.\n"
     ]
    }
   ],
   "source": [
    "print(f'Some example:\\n{comics_data[\"description\"][1]}\\n')\n",
    "print(f'Duplicated example:\\n{comics_data[\"description\"][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957d92e-6fad-41e5-b368-00a1dca68cbb",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### RNN Preprocessing (Pré-processamento RNN)\n",
    "**[EN-US]**\n",
    "\n",
    "For a sentiment classifier, we first pre-process the raw data, then tokenize our train set and extract useful features to train our model and make our predictions.\n",
    "\n",
    "Plotting the stopwords in English and the punctuations that will be removed.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Para um classificador de sentimentos, primeiro pré-processamos os dados brutos, depois, tokenizamos o nosso train set e extraímos as features úteis para treinarmos o nosso modelo e fazermos nossas previsões.\n",
    "\n",
    "Plotando as stopwords em ingês e as pontuações que serão removidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61015b4e-cfc4-4c92-b2cb-0675e5125239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuations:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "print(f'Stopwords:\\n{stopwords_en}\\n\\nPunctuations:\\n{punct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d082f99-cfc3-4a15-85b5-c1adc679eda3",
   "metadata": {},
   "source": [
    "<a name=\"2.1.1\"></a>\n",
    "#### Lowercasing, Stopwords, Stemming and Punctuations (Lowercasing, Stopwords, Stemming e Pontuações)\n",
    "**[EN-US]**\n",
    "\n",
    "After initial preprocessing of the data, we only end up with words that contain all the relevant information about the text. Initial preprocesses before splitting and tokenization:\n",
    "* `Lowercasing`: to reduce our vocabulary without losing valuable information, we will have to put each of our words in lowercase. Therefore, the word CHILDREN, Children and children, will be treated as being exactly the same word children.\n",
    "* `Special characters`: such as mathematical symbols, currency symbols, section and paragraph signs, inline markup signs and so on. It's usually safe to delete them.\n",
    "* `Stopwords and punctuation`: we remove all words that do not add significant meaning to the texts, also known as stopwords and punctuation marks. Once eliminated, the general meaning of the sentence can be inferred without any effort.\n",
    "* `Stemming`: is simply transforming any word into its base stem, which we can define as the set of characters used to build the word and its derivatives. The word works for example, its stem is `work`, because, adding the letter \"s\", it forms the word works, adding the suffix \"e\", forms the word worke, and adding the suffix \"ing\", forms the word working.\n",
    "    * After performing stemming on our corpus, the words works, worke and working will be reduced to the stem `work`. Therefore, our vocabulary will be significantly reduced by carrying out this process for each word in the corpus.\n",
    "\n",
    "Preprocessing the data, counting the initially preprocessed and duplicate examples, and plotting the first 5 examples of the initially preprocessed dataset.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Após o pré-processamento inicial dos dados, acabamos apenas com as palavras que contêm todas as informações relevantes sobre o texto. Pré-processamentos iniciais antes da divisão e tokenização:\n",
    "* `Lowercasing`: para reduzir nosso vocabulário sem perder informações valiosas, teremos que colocar cada uma de nossas palavras em lowercase. Portanto, a palavra CHILDREN, Children e children, serão tratadas como sendo exatamente a mesma palavra children.\n",
    "* `Caracteres especiais`: como símbolos matemáticos, símbolos monetários, sinais de seção e parágrafo, sinais de marcação inline e assim por diante. Geralmente é seguro excluí-los.\n",
    "* `Stopwords e pontuação`: removemos todas as palavras que não acrescentam significado significativo aos textos, também conhecidas como stopwords e sinais de pontuação (punctuation marks). Após eliminadas, o significado geral da frase pode ser inferido sem nenhum esforço.\n",
    "* `Stemming`: é simplesmente transformar qualquer palavra em sua base stem (raiz base), que podemos definir como o set de caracteres usados para construir a palavra e seus derivados. A palavra works por exemplo, seu stem é `work`, porque, adicionando a letra \"s\", forma a palavra works, adicionando o sufixo \"e\", forma a palavra worke, e adicionando o sufixo \"ing\", forma a palavra working.\n",
    "    * Depois de executar a stemming no nosso corpus, as palavras works, worke e working, serão reduzidas para a stem `work`. Portanto, nosso vocabulário será reduzido significativamente ao realizar esse processo para cada palavra do corpus.\n",
    "\n",
    "Pré-processando os dados, contando os exemplos inicialmente pré-processados e duplicados, e plotando os primeiros 5 primeiros exemplos do dataset inicialmente pré-processado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "671c01ed-8e03-4bd0-a7fe-2a160c8153b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate examples: 790\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94884</td>\n",
       "      <td>The Mighty Valkyries (2021) #3 (Variant)</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94896</td>\n",
       "      <td>X-Corp (2021) #2 (Variant)</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                     title  \\\n",
       "0  94799    Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339            The Mighty Valkyries (2021) #3   \n",
       "2  94884  The Mighty Valkyries (2021) #3 (Variant)   \n",
       "3  93350                          X-Corp (2021) #2   \n",
       "4  94896                X-Corp (2021) #2 (Variant)   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "2  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "4  shark water x corp shock debut got fenc mend h...  non-action  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data.copy()\n",
    "comics_data_pre['description'] = comics_data_pre['description'].map(rnn_preprocess)\n",
    "print(f'Number of duplicate examples: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78006e44-8e62-4182-8d93-0b3b0affd4b4",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Now we can remove the duplicate examples, because they are initially pre-processed and unscored.\n",
    "\n",
    "Plotting the number of duplicates after removing duplicates and plotting the first 5 examples from the pre-processed dataset and without duplicate examples.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Agora podemos remover os exemplos duplicados, porque estão inicialmente pré-processados e sem pontuações.\n",
    "\n",
    "Plotando o número de duplicatas após a removação das duplicatas e plotando os primeiros 5 exemplos do dataset pré-processado e sem exemplos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78ea8c4d-3a67-43ad-a3f9-c4abe0e88f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate examples: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94799</td>\n",
       "      <td>Demon Days: Mariko (2021) #1 (Variant)</td>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93339</td>\n",
       "      <td>The Mighty Valkyries (2021) #3</td>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93350</td>\n",
       "      <td>X-Corp (2021) #2</td>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93645</td>\n",
       "      <td>Heroes Reborn: Weapon X &amp; Final Flight (2021) #1</td>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93052</td>\n",
       "      <td>Heroes Reborn (2021) #6</td>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>non-action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             title  \\\n",
       "0  94799            Demon Days: Mariko (2021) #1 (Variant)   \n",
       "1  93339                    The Mighty Valkyries (2021) #3   \n",
       "3  93350                                  X-Corp (2021) #2   \n",
       "5  93645  Heroes Reborn: Weapon X & Final Flight (2021) #1   \n",
       "6  93052                           Heroes Reborn (2021) #6   \n",
       "\n",
       "                                         description           y  \n",
       "0  shadow kirisaki mountain secret histori come l...  non-action  \n",
       "1  children afterlif kraven hunter stalk jane fos...      action  \n",
       "3  shark water x corp shock debut got fenc mend h...  non-action  \n",
       "5  best world without aveng squadron suprem prote...  non-action  \n",
       "6  eon fabl daughter utopia isl known power princ...  non-action  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics_data_pre = comics_data_pre.drop_duplicates('description')\n",
    "print(f'Number of duplicate examples: {comics_data_pre[\"description\"].duplicated().sum()}')\n",
    "comics_data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff57126-3376-4b87-a05c-794d4b4003f6",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Creating a dataset with only the features that will be used in the model, transforming the target label $y$ into binary and plotting the first 5 examples of the dataset\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Criando um dataset com apenas as features que serão usadas no modelo, transformando o target label $y$ em binário e plotando os primeiros 5 exemplos do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd0398d-b2da-4eb2-bd7a-03f1342cd2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shadow kirisaki mountain secret histori come l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children afterlif kraven hunter stalk jane fos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shark water x corp shock debut got fenc mend h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>best world without aveng squadron suprem prote...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eon fabl daughter utopia isl known power princ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  y\n",
       "0  shadow kirisaki mountain secret histori come l...  0\n",
       "1  children afterlif kraven hunter stalk jane fos...  1\n",
       "3  shark water x corp shock debut got fenc mend h...  0\n",
       "5  best world without aveng squadron suprem prote...  0\n",
       "6  eon fabl daughter utopia isl known power princ...  0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the new dataset (Definindo o novo dataset)\n",
    "comics_corpus = comics_data_pre[['description', 'y']].copy()\n",
    "# Transforming the target label y into binary (Transformando o target label y em binário)\n",
    "comics_corpus['y'] = comics_corpus['y'].map(lambda x: 1 if x == 'action' else 0)\n",
    "comics_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764098-e7e0-4a26-9a2e-7cc8b4835eba",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Dividing the dataset between training, validation and testing subsets. We use the split `stratified sampling` method to try to maintain the same proportion of labels in the division between each subset, because we have a slight imbalance of classes and we don't want this to affect our model.\n",
    "\n",
    "Dividing the dataset and plotting the dimension of each of them.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Dividindo o dataset entre os subsets de treinamento, de validação e de teste. Usamos o split método `stratified sampling (amostragem estratificada)` para tentarmos manter a mesma proporção dos labels na divisão entre cada subset, porque temos um leve desbalanceamento de classes e não queremos que isso afete o nosso modelo.\n",
    "\n",
    "Dividindo o dataset e plotando a dimensão de cada um deles."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1daa5942-fecb-42ed-863e-b797ee177bee",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos_label = comics_corpus[comics_corpus['y'] == 1].copy()\n",
    "neg_label = comics_corpus[comics_corpus['y'] == 0].copy()\n",
    "pos_label_t = pos_label.iloc[:len(neg_label), :]\n",
    "corpus = pd.concat([pos_label_t, neg_label])\n",
    "\n",
    "# Splitting between training, validation and test\n",
    "# Dividindo entre treinamento, validação e teste\n",
    "train_corpus, sub_c = train_test_split(corpus, test_size=.8, shuffle=True, random_state=42)\n",
    "val_corpus, test_corpus = train_test_split(sub_c, test_size=.5, shuffle=True, random_state=42)\n",
    "print(f'Train set shape: {train_corpus.shape}\\nValidation set shape: {val_corpus.shape}\\nTest set shape: {test_corpus.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93f68470-6e08-4394-b643-b80f8e183866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (9682, 2)\n",
      "Validation set shape: (3227, 2)\n",
      "Test set shape: (3228, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting between training and the validation and testing subset\n",
    "# Dividindo entre treinamento e o subset da validação e teste\n",
    "split_train = StratifiedShuffleSplit(n_splits=1, test_size=.4, random_state=42)\n",
    "for train_index, subset_index in split_train.split(comics_corpus, comics_corpus['y']):\n",
    "    train_corpus, subset_corpus = comics_corpus.iloc[train_index, :].copy(), comics_corpus.iloc[subset_index, :].copy()\n",
    "\n",
    "# Splitting between validation and testing\n",
    "# Dividindo entre validação e teste\n",
    "split_test = StratifiedShuffleSplit(n_splits=1, test_size=.5, random_state=42)\n",
    "for val_index, test_index in split_test.split(subset_corpus, subset_corpus['y']):\n",
    "    val_corpus, test_corpus = subset_corpus.iloc[val_index, :].copy(), subset_corpus.iloc[test_index, :].copy()\n",
    "\n",
    "print(f'Train set shape: {train_corpus.shape}\\nValidation set shape: {val_corpus.shape}\\nTest set shape: {test_corpus.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9372fa-bc72-4079-8baf-84b413120439",
   "metadata": {},
   "source": [
    "Setting the global variables `VOCAB_SIZE` and `MAX_LEN` to tokenize the training set (Definindo as variáveis globais `VOCAB_SIZE` e `MAX_LEN` para tokenizar o training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18056891-e855-4c79-b132-fa93075d407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the largest clean sentence: 166\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "MAX_LEN = max([len(sentence.split()) for sentence in train_corpus['description']])\n",
    "print(f'Length of the largest clean sentence: {MAX_LEN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47db66-5cc9-4174-b0f2-af89b1ca2394",
   "metadata": {},
   "source": [
    "<a name=\"2.1.2\"></a>\n",
    "#### Tokenization (Tokenização)\n",
    "**[EN-US]**\n",
    "\n",
    "In this step, we encode our training set corpus in its vector representation, that is, first we need to create a $V$ vocabulary that allows us to encode any text as a vector of integers, for example. Where our vocabulary $V$ will be a vector of unique words from our vector of texts, where we go through each word of each text and save in the vocabulary all the new words that appear in our search. Then, we map, that is, we replace each word found in the training set with its index in the vocabulary.\n",
    "| Token | Index |\n",
    "| :---: | :---: |\n",
    "| afterlif | 1 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| balanc | 615 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| cosmic | 621 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| work | VOCAB_SIZE |\n",
    "\n",
    "The RNN can receive as input a vector representation of tokenized sentences and then we apply an embedding layer that will transform our tokenized representation of integers into values ​​that represent the semantics of that word numerically. This addresses one of the problems, the word order, alphabetical order, in this example, does not make much sense from a semantic point of view. For example, there is no reason 'cosmic' should be given a higher number than 'afterlif'.\n",
    "> I will talk more about embeddings when creating the model.\n",
    "\n",
    "This RNN will allow us to predict sentiment in complex sentences, which we would not be able to correctly classify using simpler methods like Naive Bayes because they miss important information.\n",
    "\n",
    "Our representation $X$ will be a vector of integers, that is, the index of each token in our vocabulary. Once we have all the vector representations of our sentences, we need to identify the maximum vector size and pad each vector with 0 to match that size, this process is called `padding` and it ensures that all of our vectors are the same size, even if our sentences are not.\n",
    "\n",
    "To create vocabulary, we define which words belong to the vocabulary. This means that we create a list of the words that we are going to use in our representations. One way to create this vocabulary is to look at our training set, and find the `VOCAB_SIZE` words with the most occurrence, for example, or we use already created dictionaries that tell us the `VOCAB_SIZE` words most commonly used in the language of our task.\n",
    "\n",
    "In some tasks such as speech recognition, or question answering, we will only find and generate words from a fixed set of words, for example, a chatbot can only answer limited sets of questions. This fixed list of words is also called `closed vocabulary`. However, using a fixed set of words is not always sufficient for the task. Often, we need to deal with words that we have never seen before, which results in an `open vocabulary`. **Open vocabulary** simply means that we can find words outside of vocabulary, like the name of a new city in the training set.\n",
    "\n",
    "If we train a neural network on a corpus of texts based on our vocabulary, when we want to make inference with the trained model, we will need to encode the text we want to infer with the same vocabulary. Otherwise it won't make sense, because the words would map to different numbers, different tokens. Qualquer palavra no corpus de treino que não esteja no vocabulário será substituída por `<UNK>`. Unknown words are also called `Out of vocabulary (OOV)`. Uma forma de lidar com palavras OOV é modelá-las com uma palavra especial, **\\<UNK>**. Para fazermos isso, substituímos todas as palavras OOV por \\<UNK>, o special token, `<UNK>`. The proportion of unknown words in the test set is called `OOV rate`.\n",
    "\n",
    "At the end, we apply padding, add 0 to the end of each sequence and make them all the same length. This is called `post-padding`, because the padding tokens are at the end of the sequences.\n",
    "\n",
    "Tensorflow and Keras offer us several ways to tokenize words. One of them is the layer I'm using in this system, `tensorflow.keras.layers.TextVectorization()`.\n",
    "* `tf.keras.layers.TextVectorization()`: will generate the vocabulary and create vectors from the sentences. It removes punctuations, that is, it will manage the tokens, transforming the sentences into a list of integers (the indices of each token in the vocabulary) and so on. \n",
    "* `adapt()`: method from the TextVectorization() layer, which takes the data and generates a vocabulary from the words found in these sentences.\n",
    "\n",
    "Training the tokenizer on the training set with the previously defined `VOCAB_SIZE` and `MAX_LEN` and projecting the vocabulary size.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Nessa etapa, codificamos o nosso corpus do training set em sua representação vetorial, ou seja, primeiro precisamos criar um vocabulário $V$ que nos permite codificar qualquer texto como um vetor de inteiros, por exemplo. Onde o nosso vocabulário $V$ será um vetor de palavras exclusivas do nosso vetor de textos, onde percorremos cada palavra de cada texto e salvamos no vocabulário todas as novas palavras que aparecerem em nossa pesquisa. Então, mapeamos, ou seja, substituímos cada palavra encontrada no training set pelo seu índice no vocabulário.\n",
    "| Token | Índice |\n",
    "| :---: | :----: |\n",
    "| afterlif | 1 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| balanc | 615 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| cosmic | 621 |\n",
    "| $\\vdots$ | $\\vdots$ |\n",
    "| work | VOCAB_SIZE |\n",
    "\n",
    "A RNN pode receber como input uma representação vetorial de frases tokenizadas e, então, aplicamos uma embedding layer que transformará nossa representação tokenizada de inteiros em valores que representam a semântica dessa palavra numéricamente. Isso trata um dos problemas, a ordem das palavras, ordem alfabética, neste exemplo, não faz muito sentido do ponto de vista semântico. Por exemplo, não há razão para que o 'cosmic' receba um número maior do que o 'afterlif'.\n",
    "> Falarei mais sobre embeddings na criação do modelo.\n",
    "\n",
    "Essa RNN nos permitirá prever sentimentos em frases complexas, que não conseguiríamos classificar corretamente usando métodos mais simples, como Naive Bayes, porque eles perdem informações importantes.\n",
    "\n",
    "A nossa representação $X$ será um vetor de números inteiros, ou seja, o índice de cada token do nosso vocabulário. Depois de obtermos todas as representações vetoriais das nossas frases, precisamos identificar o tamanho máximo do vetor e preencher cada vetor com 0 para corresponder a essa tamanho, esse processo é chamado de `padding` e garante que todos os nossos vetores tenham o mesmo tamanho, mesmo que nossas frases não tenham.\n",
    "\n",
    "Para a criação do vocabulário, definimos quais palavras pertencem ao vocabulário. Isso significa que criamos uma lista com as palavras que vamos usar nas nossas representações. Uma forma de criar esse vocabulário é olhar para o nosso training set, e encontrar as `VOCAB_SIZE` palavras com mais ocorrência, por exemplo, ou utilizamos dicionários já criados que nos digam as `VOCAB_SIZE` palavras mais comumente usadas na língua da nossa tarefa.\n",
    "\n",
    "Em algumas tarefas como speech recognition, ou question aswering, encontraremos e geraremos somente palavras a partir de um set fixo de palavras, por exemplo, um chatbot só pode responder em sets limitados de perguntas. Essa lista fixa de palavras também é chamada de `closed vocabulary`. No entanto, usar um set fixo de palavras nem sempre é suficiente para a tarefa. Muitas vezes, precisamos lidar com palavras que nunca vimos antes, o que resulta em um `open vocabulary`. **Open vocabulary** significa simplesmente que podemos encontrar palavras out of vocabulary (fora do vocabulário), como o nome de uma nova cidade no training set.\n",
    "\n",
    "Se treinarmos uma rede neural em um corpus de textos com base nesse nosso vocabulário, quando quisermos fazer inferência com o modelo treinado, precisaremos codificar o texto que desejamos inferir com o mesmo vocabulário. Caso contrário, não fará sentido, porque as palavras seriam mapeadas para números diferentes, tokens diferentes. Qualquer palavra no corpus de treino que não esteja no vocabulário será substituída por `<UNK>`. As unknown words também são chamadas de `Out of vocabulary (OOV)`. Uma forma de lidar com palavras OOV é modelá-las com uma palavra especial, **\\<UNK>**. Para fazermos isso, substituímos todas as palavras OOV por \\<UNK>, o special token, `<UNK>`. A proporção de unknown words no test set é chamada de `OOV rate`.\n",
    "\n",
    "No final, aplicamos o `padding`, adicionamos 0 ao final de cada sequence e fazemos com que todas tenham o mesmo comprimento. Isso é chamado de `post-padding (pós-padding)`, porque os tokens de padding estão no final das sequências.\n",
    "\n",
    "O tensorflow e o Keras nos oferecem várias maneiras de tokenizar palavras. Uma delas é a layer que estou usando nesse sistema, `tensorflow.keras.layers.TextVectorization()`.\n",
    "* `tf.keras.layers.TextVectorization()`: gerará o vocabulário e criará vetores a partir das frases. Ela retira pontuações, ou seja, ela irá gerenciar os tokens, transformando as frases em uma lista de inteiros (os índices de cada token no vocabulário) e etc. \n",
    "* `adapt()`: método da layer TextVectorization(), que pega os dados e gera um vocabulário a partir das palavras encontradas nessas frases.\n",
    "\n",
    "Treinando o tokenizador no training set com o `VOCAB_SIZE` e `MAX_LEN` definidos anteriormente e projetando o tamanho do vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cd23d50-369d-488b-9376-c900c09a591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "sentence_vec = rnn_tokenizer(train_corpus['description'], max_tokens=VOCAB_SIZE, max_len=MAX_LEN)\n",
    "print(f'Vocabulary size: {sentence_vec.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7e43b-5e54-43c3-831d-756992450d70",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Applying the trained tokenizer to each subset and plotting its dimensions and the first tokenized example of the training set.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Aplicando o tokenizador treinado em cada subset e plotando as suas dimensões e o primeiro exemplo tokenizado do training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "671f84aa-426e-4de4-9a4b-c446f81ac9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and padded train sequences shape: (9682, 166)\n",
      "\n",
      "First training padded sequence:\n",
      "[202 101  80  45 332 101  80 804 323 767   1   1 815 649 499 795 303  92\n",
      " 409 624   3   1   1   1   1 413   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "\n",
      "Tokenized and padded validation sequences shape: (3227, 166)\n",
      "Tokenized and padded test sequences shape: (3228, 166)\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = sentence_vec(train_corpus['description'])\n",
    "val_tokenized = sentence_vec(val_corpus['description'])\n",
    "test_tokenized = sentence_vec(test_corpus['description'])\n",
    "print(f'Tokenized and padded train sequences shape: {train_tokenized.shape}\\n\\nFirst training padded sequence:\\n{train_tokenized[0]}\\n')\n",
    "print(f'Tokenized and padded validation sequences shape: {val_tokenized.shape}\\nTokenized and padded test sequences shape: {test_tokenized.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962e42e-a0f3-4770-9218-ed41877a2d4f",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Loading the trained tokenization model into the `../models/` directory for later use. We save the hyperparameters that were used in training and the generated vocabulary.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Carregando o modelo de tokenização treinado no diretório `../models/` para usarmos posteriormente. Salvamos os hiperparâmetros que foram utilizados no treinamento e o vocabulário gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e20a20-0271-4fa3-9dbf-78da1a2d6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    {'config': sentence_vec.get_config(), 'vocabulary': sentence_vec.get_vocabulary()},\n",
    "    open('../models/vectorizer.pkl', 'wb')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32031-b879-4358-b2a8-22a7ab39427e",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Transforming the $y$ labels into a column vector, concatenating each tokenized corpus with the corresponding $y$ labels and plotting the dimension of each subset.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Transformando os labels $y$ em um vetor de coluna, concatenando cada corpus tokenizado com os labels $y$ correspondentes e plotando a dimensão de cada subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0738867-7ac3-4312-aff8-26a8eefea2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed train set shape: (9682, 167)\n",
      "Preprocessed validation set shape: (3227, 167)\n",
      "Preprocessed test set shape: (3228, 167)\n"
     ]
    }
   ],
   "source": [
    "# Transforming the y labels into a column vector\n",
    "# Transformando os labels y em um vetor de coluna\n",
    "labels_train = train_corpus[['y']].copy()\n",
    "labels_val = val_corpus[['y']].copy()\n",
    "labels_test = test_corpus[['y']].copy()\n",
    "\n",
    "# Concatenating the corpus of each subset and the corresponding labels\n",
    "# Concatenando o corpus de cada subset e os labels correspondentes\n",
    "train_tokens = np.concatenate([train_tokenized, labels_train], axis=1)\n",
    "val_tokens = np.concatenate([val_tokenized, labels_val], axis=1)\n",
    "test_tokens = np.concatenate([test_tokenized, labels_test], axis=1)\n",
    "print(f'Preprocessed train set shape: {train_tokens.shape}\\nPreprocessed validation set shape: {val_tokens.shape}\\nPreprocessed test set shape: {test_tokens.shape}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10a34b7f-7c1c-4144-baa6-6f0fe4b4e26c",
   "metadata": {},
   "source": [
    "# Transforming the y labels into a column vector\n",
    "# Transformando os labels y em um vetor de coluna\n",
    "labels_pre = comics_corpus[['y']].copy()\n",
    "\n",
    "# Concatenating the corpus of each subset and the corresponding labels\n",
    "# Concatenando o corpus de cada subset e os labels correspondentes\n",
    "train_tokens = np.concatenate([train_tokenized, labels_pre[train_index]], axis=1)\n",
    "valid_tokens = np.concatenate([valid_tokenized, labels_pre[valid_index]], axis=1)\n",
    "test_tokens = np.concatenate([test_tokenized, labels_pre[test_index]], axis=1)\n",
    "print(f'Preprocessed train set shape: {train_tokens.shape}\\nPreprocessed validation set shape: {valid_tokens.shape}\\nPreprocessed test set shape: {test_tokens.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af5b1-21f5-411a-bfbf-77385bdf293f",
   "metadata": {},
   "source": [
    "Loading each preprocessed dataset into the `../data/preprocessed/` directory (Carregando cada dataset pré-processado no diretório `../data/preprocessed/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5444967e-b6d0-4877-853c-10d2e41d6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset with initial pre-processing to disk\n",
    "# Carregando no disco o dataset com pré-processamento inicial\n",
    "comics_corpus.to_csv('../data/preprocessed/comics_corpus.csv', index=False)\n",
    "\n",
    "# Loading tokenized datasets to disk\n",
    "# Carregando no disco os datasets tokenizados\n",
    "np.save('../data/preprocessed/train_tokens.npy', train_tokens)\n",
    "np.save('../data/preprocessed/validation_tokens.npy', val_tokens)\n",
    "np.save('../data/preprocessed/test_tokens.npy', test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60216a38-23ab-4aad-bc3f-99f5195e81ad",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### Transformer Preprocessing (Pré-processamento Transformers)\n",
    "**[EN-US]**\n",
    "\n",
    "For transformers preprocessing, we will use a pre-trained tokenizer from the `DistilBERT` checkpoint to apply tokenization and padding.\n",
    "\n",
    "DistilBERT is a small, fast, cheap and lightweight Transformers model trained by distilling the BERT (Bidirectional Encoder Representation from Transformers) base model. It has 40% fewer parameters than bert-base-uncased, runs 60% faster, and preserves more than 95% of Bert's performance as measured in the GLUE (General Language Understanding Evaluation) benchmark.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (🤗) is the best resource for pre-trained transformers. Its open-source libraries make it simple to download, fine-tune, and use transformer models like DeepSeek, BERT, Llama, T5, Qwen, GPT-2, and more. And the best part, you can use them together with TensorFlow, PyTorch or Flax. In this system, I use 🤗 transformers to use the `DistilBERT` model for sentiment classification. For the pre-processing step, we used the pre-trained DistilBERT tokenizer `distilbert-base-uncased-finetuned-sst-2-english`, for this we initialized the DistilBertTokenizer class and defined the desired pre-trained model.\n",
    "> In the fine-tuning of the model, in the notebook `05_transformers_finetuning.ipynb`, I will talk in more detail about transfer learning and fine-tuning.\n",
    "\n",
    "Tokenizing, padding the corpus and returning the tokenized corpus as pytorch tensors using the pre-trained `DistilBERT` tokenizer. Defining the tokenizer and plotting the vector representations of the corpus.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Para o pré-processamento do transformers, vamos utilizar um tokenizador pré-treinado do checkpoint do `DistilBERT` para aplicar a tokenização e o padding.\n",
    "\n",
    "DistilBERT é um modelo Transformers pequeno, rápido, barato e leve treinado pela destilação do modelo base BERT (Bidirectional Encoder Representation from Transformers). Ele tem 40% menos parâmetros que o bert-base-uncased, roda 60% mais rápido e preserva mais de 95% do desempenho do Bert conforme medido no benchmark GLUE (General Language Understanding Evaluation).\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (🤗) é o melhor recurso para transformers pré-treinados. Suas bibliotecas open-source simplificam o download, o fine-tuning e o uso de modelos de transformers como DeepSeek, BERT, Llama, T5, Qwen, GPT-2 e muito mais. E a melhor parte, você pode usá-los junto com TensorFlow, PyTorch ou Flax. Neste sistema, utilizo transformers 🤗 para usar o modelo `DistilBERT` para classificação de sentimento. Para a etapa do pré-processamento, usamos o tokenizador do DistilBERT `distilbert-base-uncased-finetuned-sst-2-english` pré-treinado, para isso inicializamos a classe DistilBertTokenizer e definidos o modelo pré-treinado desejado.\n",
    "> No fine-tuning do modelo, no notebook `05_transformers_finetuning.ipynb`, falarei com mais detalhes sobre o transfer learning e o fine-tuning.\n",
    "\n",
    "Tokenizando, aplicando o padding no corpus e retornando o corpus tokenizado como tensores pytorch utilizando o tokenizer `DistilBERT` pré-treinado. Definindo o tokenizer e plotando as representações vetoriais do corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4600970c-28f4-4ea8-9ffb-a28b511145cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1999,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  2336,  1997,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1037,  3181,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  2028,  ...,     0,     0,     0],\n",
       "        [  101, 16228,  2023,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the pre-trained model\n",
    "# Definindo o modelo pré-treinado\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "# Defining the tokenizer\n",
    "# Definindo o tokenizer\n",
    "comics_transformers = tokenizer(\n",
    "    comics_data['description'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "# Accessing the vector representations of the corpus\n",
    "# Acessando as representações vetoriais do corpus\n",
    "transformers_tokens = comics_transformers['input_ids']\n",
    "transformers_attention = comics_transformers['attention_mask']\n",
    "transformers_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7d949-f92d-47ea-b8f2-b004af97ba74",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Selecting the labels from the raw dataset and dividing the tokenized tensor between the training, validation and testing subsets. We use the `stratified sampling` split method to try to maintain the same proportion of labels in the division between each subset.\n",
    "\n",
    "Dividing the dataset and plotting the size of each of them.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Selecionando os labels do dataset bruto e dividindo o tensor tokenizado entre os subsets de treinamento, de validação e de teste. Usamos o método de split `stratified sampling (amostragem estratificada)` para tentarmos manter a mesma proporção dos labels na divisão entre cada subset.\n",
    "\n",
    "Dividindo o dataset e plotando o tamanho de cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97f49488-f556-47c9-bcb9-33ac7988ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset size: 10156\n",
      "Validation subset size: 3385\n",
      "Test subset size: 3386\n"
     ]
    }
   ],
   "source": [
    "# Selecting labels from the raw dataset\n",
    "# Selecionando os labels do dataset bruto\n",
    "labels = (comics_data['y']\n",
    "          .map(lambda x: 1 if x == 'action' else 0)\n",
    "          .to_numpy()\n",
    "          .reshape(-1, 1))\n",
    "\n",
    "# Splitting between training and the validation and testing subset\n",
    "# Dividindo entre treinamento e o subset da validação e teste\n",
    "train_idx, subset_idx = next(split_train.split(transformers_tokens, labels))\n",
    "# Splitting between validation and testing\n",
    "# Dividindo entre validação e teste\n",
    "val_idx, test_idx = next(split_test.split(transformers_tokens[subset_idx], labels[subset_idx]))\n",
    "print(f'Train subset size: {len(train_idx)}\\nValidation subset size: {len(val_idx)}\\nTest subset size: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba41bb9-9768-4121-8ebc-e982a80102b8",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Transforming the tokenized pytorch tensor subsets divided by the `stratified sampling split` to the `Dataset` type in dictionary format, to perform fine-tuning of the transformers and plotting each dataset.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Transformando os subsets de tensores pytorch tokenizados divididos pela `stratified sampling split` para o tipo `Dataset` em formato de dicionário, para executarmos o fine-tuning do transformers e plotando cada dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b657d0b8-5c4c-413e-aa4c-2aa37c8e134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Validation dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, train_idx)\n",
    "val_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, val_idx)\n",
    "test_dataset = tensors_to_dataset(transformers_tokens, transformers_attention, labels, test_idx)\n",
    "print(f'Train dataset: {train_dataset}\\nValidation dataset: {val_dataset}\\nTest dataset: {test_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7634f8-e5e5-4a58-bbd0-289af0787473",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Loading each pre-processed dataset and its metadata into its specific directory within the `../data/preprocessed/` directory.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Carregando cada dataset pré-processado e seus metadados dentro de seu diretório específico dentro do diretório `../data/preprocessed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d767741-02a8-493a-a30d-f1ebec36b2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3278c9f7c64bb88eded1df5b529eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10156 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf2b8fe7fed4753b4811cd6fb13f290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3385 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7784715774b4fde89c017ddbf2206fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('../data/preprocessed/train_dataset')\n",
    "val_dataset.save_to_disk('../data/preprocessed/validation_dataset')\n",
    "test_dataset.save_to_disk('../data/preprocessed/test_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
