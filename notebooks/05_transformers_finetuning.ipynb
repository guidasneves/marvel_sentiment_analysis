{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f339ad48-c1bf-4e5c-83c0-e73d2fc7a296",
   "metadata": {},
   "source": [
    "# Transformers Fine-tuning Step (Etapa do Fine-tuning do Transformers)\n",
    "**[EN-US]**\n",
    "\n",
    "Fine-tuning step in the DistilBERT model pre-trained on our data, to later save the model and its respective trained weights, and use it to make predictions on real-world data.\n",
    "\n",
    "In this step we perform:\n",
    "* Model definition;\n",
    "* Fine-tuning the model;\n",
    "* Model evaluation;\n",
    "* Saving the model and trained weights.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Etapa do fine-tuning no modelo DistilBERT pr√©-treinado nos nossos dados, para depois, salvarmos o modelo e seus respectivos pesos treinados, e usarmos para realizar a predi√ß√£o em dados do mundo real.\n",
    "\n",
    "Nesta etapa realizamos:\n",
    "* Defini√ß√£o do modelo;\n",
    "* Fine-tuning do modelo;\n",
    "* Avalia√ß√£o do modelo;\n",
    "* Salvamento do modelo e dos pesos treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6776b-a00c-4755-8110-bfdf4188cc94",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Packages](#1)\n",
    "* [Loading the Data](#2)\n",
    "* [Transfer Learning](#3)\n",
    "    * [Fine-tuning](#3.1)\n",
    "* [Transformers](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf3166-e70e-4490-a37b-1c681cf8966d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Packages (Pacotes)\n",
    "**[EN-US]**\n",
    "\n",
    "Packages used in the system.\n",
    "* [numpy](www.numpy.org): is the main package for scientific computing;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): provides APIs and tools to easily download and train state-of-the-art pretrained models.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Pacotes utilizados no sistema.\n",
    "* [numpy](www.numpy.org): √© o principal pacote para computa√ß√£o cient√≠fica;\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): fornece APIs e ferramentas para baixar e treinar facilmente modelos pr√©-treinados de √∫ltima gera√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06cc8d8e-b767-4962-a5a7-e7dea0012b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead2267-48de-4889-aac5-dea58282b080",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Loading the Data (Carregando os Dados)\n",
    "**[EN-US]**\n",
    "\n",
    "We will read each of the subsets from the `../data/preprocessed/` directory and plotting their dimensions.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Vamos ler cada um dos subsets do diret√≥rio `../data/preprocessed/` e plotando as suas dimens√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e38bf67-f145-47b7-bfa7-5149743df283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (9682, 252)\n",
      "Validation set shape: (3227, 252)\n",
      "Test set shape: (3228, 252)\n"
     ]
    }
   ],
   "source": [
    "files = ['train', 'valid', 'test']\n",
    "datasets = []\n",
    "for file in files:\n",
    "    with open(f'../data/preprocessed/{file}_transformers.npy', 'rb') as f:\n",
    "        datasets.append(np.load(f))\n",
    "\n",
    "train_corpus, valid_corpus, test_corpus = datasets\n",
    "print(f'Train set shape: {train_corpus.shape}\\nValidation set shape: {valid_corpus.shape}\\nTest set shape: {test_corpus.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a315b-c133-4fac-a7f8-f175550ad4e2",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## Transfer Learning (Aprendizado por Transfer√™ncia)\n",
    "**[EN-US]**\n",
    "\n",
    "One of the most powerful ideas in deep learning is that sometimes we can take the knowledge that the neural network learned in one task and apply that knowledge to a separate task. There are 3 main advantages to transfer learning:\n",
    "* Reduces training time.\n",
    "* Improves predictions.\n",
    "* Allows us to use smaller datasets.\n",
    "\n",
    "If we are creating a model, rather than training the weights from 0, from a random initialization, we often make much faster progress by downloading weights that someone else has already trained for days/weeks/months on the neural network architecture, use them as pre-training, and transfer them to a new, similar task that we might be interested in. This means that we can often download weights from open-source algorithms that other people took weeks or months to figure out, and then we use that as a really good initialization for our neural network.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Uma das ideias mais poderosas do deep learning, que √†s vezes, podemos pegar o conhecimento que a rede neural aprendeu em uma tarefa e aplicar esse conhecimento em uma tarefa separada. H√° 3 vantagens principais no transfer learning:\n",
    "* Reduz o tempo de treinamento.\n",
    "* Melhora as previs√µes.\n",
    "* Nos permite usar datasets menores.\n",
    "\n",
    "Se estamos criando um modelo, em vez de treinarmos os pesos do 0, a partir de uma inicializa√ß√£o aleat√≥ria, geralmente progredimos muito mais r√°pido baixando pesos que outra pessoa j√° treinou por dias/semanas/meses na arquitetura da rede neural, as usamos como pr√©-treinamento, e as transferimos para uma nova tarefa semelhante na qual possamos estar interessados. Isso significa que muitas vezes podemos baixar pesos de algoritmos open-source, que outras pessoas levaram semanas ou meses para descobrir, e ent√£o, usamos isso como uma inicializa√ß√£o muito boa para nossa rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9148a-39fe-4150-bc54-7976a67f4e45",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### Fine-tuning\n",
    "**[EN-US]**\n",
    "\n",
    "We take the weights from an existing pre-trained model using transfer learning and then tweak them a bit to ensure they work for the specific task we are working on. Let's say we pre-trained a model that predicts movie evaluation, and now we're going to create a model to evaluate courses. One way to do this is, by locking all the weights that we already have pre-trained, and then we add a new output layer, or perhaps, a new feed forward layer and an output layer that will be trained, while we keep the rest locked and then we only train our new network, the new layers that we just added. We can slowly unfreeze the layers, one at a time.\n",
    "\n",
    "Many of the low-level features that the pre-trained model learned from a very large corpus, like the structure of the text, the nature of the text, this can help our algorithm do better in the sentiment classification task and faster or with less data, because maybe the model has learned enough what the structures of different texts are like and some of that knowledge will be useful. After deleting the output layer of a pre-trained model, we don't necessarily need to create just the output layer, but we can create several new layers.\n",
    "\n",
    "We need to remove the output layer from the pre-trained model and add ours, because the neural network can have a softmax output layer that generates one of 1000 possible labels. So we remove this output layer and create our own output layer, in this case a sigmoid activation.\n",
    "\n",
    "* With a small training set, we think of the rest of the layers as `frozen`, so we freeze the parameters of these layers, and only train the parameters associated with our output layer. This way we will obtain very good performance, even with a small training set.\n",
    "\n",
    "* With a larger training set, we can freeze fewer layers and then train the layers that were not frozen and our new output layer. We can use the layers that are not frozen as initialization and use gradient descent from them, or we can also eliminate these layers that are not frozen and use our own new hidden layers and our own output layer. Any of these methods could be worth trying.\n",
    "\n",
    "* With a much larger training set, we use this pre-trained neural network and its weights as initialization and train the entire neural network, just changing the output layer, with labels that we care about.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Pegamos os pesos de um modelo pr√©-treinado existente, utilizando transfer learning e, em seguida, ajustamos um pouco para garantir que funcionem na tarefa espec√≠fica em que estamos trabalhando. Digamos que pr√©-treinamos um modelos que prev√™ a avalia√ß√£o de filmes, e agora vamos criar um modelo para avaliar cursos. Uma maneira de fazer isso √©, bloqueando todos os pesos que j√° temos pr√©-treinados e, em seguida, adicionamos uma nova output layer, ou talvez, uma nova feed forward layer e uma output layer que ser√£o treinadas, enquanto mantemos o restante bloqueado e, em seguida, treinamos apenas a nossa nova rede, as novas layers que acabamos de adicionar. Podemos descongelar lentamente as layers, uma de cada vez.\n",
    "\n",
    "Muitas das features de baixo n√≠vel que o modelo pr√©-treinado aprendeu a partir de um corpus muito grande, como a estrutura do texto, a natureza do texto, isso pode ajudar nosso algoritmo a se sair melhor na tarefa de classifica√ß√£o de sentimentos e mais r√°pido ou com menos dados, porque talvez o modelo tenha aprendido o suficiente como s√£o as estruturas de textos diferentes e parte desse conhecimento ser√° √∫til. Ap√≥s excluirmos a output layer de um modelo pr√©-treinado, n√£o precisamos necessariamente criar apenas a output layer, mas podemos criar v√°rias novas layers.\n",
    "\n",
    "Precisamos remover a output layer do modelo pr√©-treinado e adicionar a nossa, porque a rede neural pode ter uma softmax output layer que gera um dos 1000 labels poss√≠veis. Ent√£o removemos essa output layer e criamos a nossa pr√≥pria output layer, nesse caso uma ativa√ß√£o sigmoid.\n",
    "\n",
    "* Com um training set pequeno, pensamos no restante das layers como `congeladas`, ent√£o congelamos os par√¢metros dessas layers, e treinamos apenas os par√¢metros associados √† nossa output layer. Dessa forma obteremos um desempenho muito bom, mesmo com um training set pequeno.\n",
    "\n",
    "* Com um training set maior, n√≥s podemos congelar menos layers e ent√£o treinar as layers que n√£o foram congeladas e a nossa nova output layer. Podemos usar as layers que n√£o est√£o congeladas como inicializa√ß√£o e usar o gradient descent a partir delas, ou tamb√©m podemos eliminar essas layers que n√£o est√£o congeladas e usamos nossas pr√≥prias hidden layers novas e nossa pr√≥pria output layer. Qualquer um desses m√©todos pode valer a pena tentar.\n",
    "\n",
    "* Com um training set muito maior usamos essa rede neural pr√©-treinada e os seus pesos como inicializa√ß√£o e treinamos toda a rede neural, apenas alterando a output layer, com labels que nos importamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37646c-d5a8-4829-a7d7-76080f57e6d3",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## Transformers\n",
    "**[EN-US]**\n",
    "\n",
    "<img align='center' src='../figures/transformers.png' style='width:400px;'>\n",
    "Transformer is a purely attention-based model that was developed by Google to solve some problems with RNNs, it is difficult to fully exploit the advantages of parallel computing, due to its problems arising from its sequential structure. In a seq2seq RNN, we need to go through each word of our input, sequentially by the encoder, and it is done in a similar sequential way by the decoder, without parallel computation. For this reason, there is not much room for parallel calculations. The more words we have in the input sequence, the more time it will take to process it.\n",
    "\n",
    "With large sequences, that is, with many $T$ sequential steps, information tends to get lost in the network (loss of information) and problems of vanishing gradients arise related to the length of our input sequences. LSTMs and GRUs help a little with these problems, but even these architectures stop working well when they try to process very long sequences due to the `information bottleneck`.\n",
    "* `Loss of information`: it is more difficult to know whether the subject is singular or plural as we move away from the subject.\n",
    "* `Vanishing gradients`: When we calculate backprop, the gradients can become very small and as a result the model will not learn anything.\n",
    "\n",
    "Transformers are based on attention and do not require any sequential calculation per layer, requiring only a single step. Furthermore, the gradient steps that need to be taken from the last output to the first output in a transformer are just 1. For RNNs, the number of steps increases with longer sequences. Finally, transformers do not suffer from problems of vanishing gradients related to the length of the sequences.\n",
    "\n",
    "Transformers do not use RNNs, attention is all we need, and only a few linear and non-linear transformations are usually included. The transformers model was introduced in 2017 by Google researchers, and since then, the transformer architecture has become standard for LLMs. Transformers have revolutionized the field of NLP.\n",
    "\n",
    "The transformer model uses `scaled dot-product attention`. The first form of attention is very efficient in terms of computation and memory, because it consists only of matrix multiplication operations. This engine is the core of the model and allows the transfomer to grow and become more complex, while being faster and using less memory than other comparable model architectures.\n",
    "\n",
    "In the transformer model, we will use the `Multi-Head Attention layer`, this layer is executed in parallel and has several $h$ scaled dot-product attention mechanisms and several linear transformations of input queries, keys and values. In this layer, linear transformations are trainable parameters.\n",
    "$$\\text{ Attention}(Q, K, V) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V$$\n",
    "<img align='center' src='../figures/attention.png' style='width:600px;'>\n",
    "\n",
    "`Encoders` transformers start with a multi-head attention module that performs `self-attention` on the input sequence. This is followed by a residual connection and normalization, then a feed forward layer and another residual connection and normalization. The encoder layer is repeated $N_x$ times.\n",
    "* Self-attention: each word in the input corresponds to each other word in the input sequence\n",
    "* Thanks to the self-attention layer, the encoder will provide a contextual representation of each of our inputs.\n",
    "\n",
    "\n",
    "The `decoder` is built in a similar way to the encoder, with multi-head attention, residual connections and normalization modules. The first attention module is masked (`Masked Self-Attention`) so that each position only serves the previous positions, it blocks the flow of information to the left. The second attention module (`Encoder-Decoder Attention`) takes the encoder output and allows the decoder to attend to all items. This entire decoder layer is also repeated several $N_x$ times, one after the other.\n",
    "$$\\text{ Masked Self-Attention } = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + M \\right) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + \\text{ mask matrix } \\begin{pmatrix} 0 & -\\infty & -\\infty \\\\ 0 & 0 & -\\infty \\\\ 0 & 0 & 0 \\end{pmatrix} \\right)$$\n",
    "\n",
    "Transformers also incorporate a `positional encoding stage` ($PE$), which encodes the position of each input in the sequence, that is, the sequential information. This is necessary because transformers don't use RNNs, but word order is relevant to any language. Positional encoding is trainable, just like word embeddings.\n",
    "$$\\begin{align*}\n",
    "& \\text{PE}_{(\\text{pos, }2i)} = \\text{sin}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right) \\\\\n",
    "& \\text{PE}_{(\\text{pos, }2i + 1)} = \\text{cos}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "First, the embedding on the input is calculated and the positional encodings are applied. So, this goes to the encoder which consists of several layers of Multi-Head Attention modules, then the decoder receives the output sequence shifted one step to the right and the encoder outputs. The decoder output is transformed into output probabilities using a linear layer with a softmax activation. This architecture is easy to parallelize compared to RNNs models and can be trained much more efficiently on multiple GPUs. It can also be scaled to learn multiple tasks on increasingly larger datasets. Transformers are a great alternative to RNNs and help overcome these problems in NLP and many fields that process sequential data.\n",
    "\n",
    "We will fine-tune the DistilBERT model, which is a small, fast, cheap and lightweight Transformer model, trained by distilling the base BERT model. It has 40% fewer parameters than bert-base-uncased, runs 60% faster, and preserves more than 95% of BERT's performance as measured in the GLUE (General Language Understanding Evaluation) benchmark.\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (ü§ó) is the best resource for pre-trained transformers. Its open source libraries make it simple to download, fine-tune, and use transformer models like DeepSeek, BERT, Llama, T5, Qwen, GPT-2, and more. And the best part, we can use them together with TensorFlow, PyTorch or Flax. In this notebook, I use ü§ó transformers to use the `DistilBERT` model for sentiment classification. For the pre-processing step, we use the tokenizer (in the notebook `03_preprocessing.ipynb`), and the DistilBERT fine-tuning `distilbert-base-uncased-finetuned-sst-2-english` pre-trained in the code below. To do this, we initialize the DistilBertForSequenceClassification class and define the desired pre-trained model.\n",
    "\n",
    "Defining the pre-trained model that we will do the fine-tuning.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "<img align='center' src='../figures/transformers.png' style='width:400px;'>\n",
    "Transformer √© um modelo puramente baseado em attention que foi desenvolvido pelo Google para solucionar alguns problemas com RNNs, √© dif√≠cil de explorar totalmente as vantagens da computa√ß√£o paralela, devido aos seus problemas decorrentes de sua estrutura sequencial. Em um RNN seq2seq, precisamos passar por cada palavra de nosso input, de forma sequencial pelo encoder, e √© feito de uma forma sequencial similar pelo decoder, sem computa√ß√£o paralela. Por esse motivo, n√£o h√° muito espa√ßo para c√°lculos paralelos. Quanto mais palavras temos na sequ√™ncia de input, mais tempo ser√° necess√°rio para process√°-la.\n",
    "\n",
    "Com sequ√™ncias grandes, ou seja, com muitos $T$ steps sequenciais, as informa√ß√µes tendem a se perder na rede (loss of information) e problemas de vanishing gradients surgem relacionados ao comprimento de nossas input sequences. LSTMs e GRUs ajudam um pouco com esses problemas, mas mesmo essas arquitutras param de funcionar bem quando tentam processar sequ√™ncias muito longas devido ao `information bottleneck`.\n",
    "* `Loss of information`: √© mais dif√≠cil saber se o sujeito √© singular ou plural √† medida que nos afastamos do sujeito.\n",
    "* `Vanishing gradients`: quando calculamos o backprop, os gradients podem se tornar muito pequenos e, como resultado, o modelo n√£o aprender√° nada.\n",
    "\n",
    "Os transformers se baseiam em attention e n√£o exigem nenhum c√°lculo sequencial por layer, sendo necess√°rio apenas um √∫nico step. Al√©m disso, os steps de gradient que precisam ser realizados do √∫ltimo output para o primeiro output em um transformer s√£o apenas 1. Para RNNs, o n√∫mero de steps aumenta com sequ√™ncias mais longas. Por fim, os transformers n√£o sofrem com problemas de vanishing gradients relacionados ao comprimento das sequ√™ncias.\n",
    "\n",
    "Transformers n√£o usam RNNs, attention √© tudo o que precisamos, e apenas algumas transforma√ß√µes lineares e n√£o lineares geralmente s√£o inclu√≠das. O modelo transformers foi introduzido em 2017 por pesquisadores do Google, e desde ent√£o, a arquitetura do transformer se tornou padr√£o para LLMs. Os transformers revolucionaram o campo de NLP.\n",
    "\n",
    "O modelo transformer usa `scaled dot-product attention`. A primeira forma da attention √© muito eficiente em termos de computa√ß√£o e mem√≥ria, porque consiste apenas em opera√ß√µes de multilpica√ß√µes de matrizes. Esse mecanismo √© o n√∫cleo do modelo e permite que o transfomer cres√ßa e se torne mais complexo, sendo mais r√°pido e usando menos mem√≥ria do que outras arquiteturas de modelos compar√°veis.\n",
    "\n",
    "No modelo transformer, usaremos a `Multi-Head Attention layer`, essa layer √© executada em paralelo e tem v√°rios mecanismos $h$ scaled dot-product attention e v√°rias transforma√ß√µes lineares dos input queries, keys e values. Nessa layer, as transforma√ß√µes lineares s√£o par√¢metros trein√°veis.\n",
    "$$\\text{ Attention}(Q, K, V) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V$$\n",
    "<img align='center' src='../figures/attention.png' style='width:600px;'>\n",
    "\n",
    "Os transformers `encoders` come√ßam com um m√≥dulo multi-head attention que executa a `self-attention` na input sequence. Isso √© seguido por uma residual connection e normaliza√ß√£o, em seguida, uma feed forward layer e outra residual connection e normaliza√ß√£o. A encoder layer √© repetida $N_x$ vezes.\n",
    "* Self-attention: cada palavra no input corresponde a cada outra palavra no input sequence\n",
    "* Gra√ßas √† self-attention layer, o encoder fornecer√° uma representa√ß√£o contextual de cada um de nossos inputs.\n",
    "\n",
    "\n",
    "O `decoder` √© constru√≠do de forma similar ao encoder, com m√≥dulos multi-head attention, residual connections e normaliza√ß√£o. O primeiro m√≥dulo de attention √© mascarado (`Masked Self-Attention`) de forma que cada posi√ß√£o atenda apenas √†s posi√ß√µes anteriores, ele bloqueia o fluxo de informa√ß√µes para a esquerda. O segundo m√≥dulo de attention (`Encoder-Decoder Attention`) pega o output do encoder e permite que o decoder atenda a todos os itens. Toda essa decoder layer tamb√©m √© repetida v√°rias $N_x$ vezes, uma ap√≥s a outra.\n",
    "$$\\text{ Masked Self-Attention } = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + M \\right) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + \\text{ mask matrix } \\begin{pmatrix} 0 & -\\infty & -\\infty \\\\ 0 & 0 & -\\infty \\\\ 0 & 0 & 0 \\end{pmatrix} \\right)$$\n",
    "\n",
    "Os transformers tamb√©m incorporam um `positional encoding stage` ($PE$), que codifica a posi√ß√£o de cada input na sequ√™ncia, ou seja, as informa√ß√µes sequenciais. Isso √© necess√°rio porque os transformers n√£o usam RNNs, mas a ordem das palavras √© relevante para qualquer idioma. A positional encoding √© trein√°vel, assim como as word embeddings.\n",
    "$$\\begin{align*}\n",
    "& \\text{PE}_{(\\text{pos, }2i)} = \\text{sin}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right) \\\\\n",
    "& \\text{PE}_{(\\text{pos, }2i + 1)} = \\text{cos}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Primeiro, √© calculado o embedding sobre o input e as positional encodings s√£o aplicadas. Ent√£o, isso vai para o encoder que consiste em v√°rias layers de m√≥dulos de Multi-Head Attention, em seguida, o decoder recebe a sequ√™ncia de output deslocada um step para a direita e os outputs do encoder. O output do decoder √© transformado em probabilidades de outputs usando uma layer linear com uma ativa√ß√£o softmax. Essa arquitetura √© f√°cil de paralelizar em compara√ß√£o com os modelos RNNs e pode ser treinada com muito mais efici√™ncia em v√°rias GPUs. Ela tamb√©m pode ser escalada para aprender v√°rias tarefas em datasets cada vez maiores. Transformers s√£o uma √≥tima alternativa aos RNNs e ajudam a superar esses problemas em NLP e em muitos campos que processam dados sequenciais.\n",
    "\n",
    "Faremos o fine-tuning no modelo DistilBERT, que √© um modelo Transformer pequeno, r√°pido, barato e leve, treinado pela destila√ß√£o do modelo base BERT. Ele tem 40% menos par√¢metros que o bert-base-uncased, roda 60% mais r√°pido e preserva mais de 95% do desempenho do BERT conforme medido no benchmark GLUE (General Language Understanding Evaluation).\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (ü§ó) √© o melhor recurso para transformers pr√©-treinados. Suas bibliotecas de c√≥digo aberto simplificam o download, o fine-tuning e o uso de modelos de transformers como DeepSeek, BERT, Llama, T5, Qwen, GPT-2 e muito mais. E a melhor parte, podemos us√°-los junto com TensorFlow, PyTorch ou Flax. Neste notebook, utilizo transformers ü§ó para usar o modelo `DistilBERT` para classifica√ß√£o de sentimento. Para a etapa do pr√©-processamento, usamos o tokenizador (no notebook `03_preprocessing.ipynb`), e no fine-tuning do DistilBERT `distilbert-base-uncased-finetuned-sst-2-english` pr√©-treinado no c√≥digo abaixo. Para isso inicializamos a classe DistilBertForSequenceClassification e definidos o modelo pr√©-treinado desejado.\n",
    "\n",
    "Definindo o modelo pr√©-treinado que faremos o fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95b6dd67-2714-4583-81d0-f8db9e01fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e5df1-9a50-43d1-b08b-da5943921f30",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Setting the hyperparameters, using the Hugging Face Trainer object to fine-tune the model.\n",
    "* The pre-trained model with fine-tuning is already being saved in the `../models/` directory, defined when defining the hyperparameters.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Definindo os hiperpar√¢metros, usando o objeto Trainer do Hugging Face para realizar o fine-tuning do modelo.\n",
    "* O modelo pr√©-treinado com o fine-tuning j√° est√° sendo salvo no diret√≥rio `../models/`, definido na defini√ß√£o dos hiperpar√¢metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585ffdb-779b-420d-81f3-65fbe73251af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning hyperparameters\n",
    "# Hiperpar√¢metros do fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/transformers_results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "# Objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_corpus,\n",
    "    eval_dataset=valid_corpus,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bfb82-0c3e-430d-b4e4-5debe24f6ca5",
   "metadata": {},
   "source": [
    "**[EN-US]**\n",
    "\n",
    "Evaluating model performance with fine-tuning in the test set.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Avaliando o desempenho do modelo com fine-tuning no test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3b133-8ef9-46c0-9a67-517822821220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test set evaluate: {trainer.evaluate(test_corpus)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
