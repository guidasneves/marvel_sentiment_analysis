{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f339ad48-c1bf-4e5c-83c0-e73d2fc7a296",
   "metadata": {},
   "source": [
    "# Etapa do Fine-tuning do Transformers\n",
    "Etapa do fine-tuning no modelo DistilBERT pr√©-treinado nos nossos dados, para depois, salvarmos o modelo e seus respectivos pesos treinados.\n",
    "\n",
    "Nesta etapa realizamos:\n",
    "* Defini√ß√£o do modelo;\n",
    "* Fine-tuning do modelo;\n",
    "* Avalia√ß√£o do modelo;\n",
    "* Salvamento do modelo e dos pesos treinados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6776b-a00c-4755-8110-bfdf4188cc94",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Pacotes](#1)\n",
    "* [Carregando os Dados](#2)\n",
    "* [Transformers](#3)\n",
    "* [Transfer Learning](#4)\n",
    "    * [Fine-tuning](#4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf3166-e70e-4490-a37b-1c681cf8966d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Pacotes\n",
    "Pacotes que foram utilizados no sistema:\n",
    "* [transformers](https://huggingface.co/docs/transformers/index): fornece APIs e ferramentas para baixar e treinar facilmente modelos pr√©-treinados de √∫ltima gera√ß√£o;\n",
    "* [datasets](https://huggingface.co/docs/datasets/index): √© uma biblioteca para acessar e compartilhar facilmente datasets para tarefas de √°udio, vis√£o computacional e processamento de linguagem natural (NLP);\n",
    "* [scikit-learn](https://scikit-learn.org/stable/): biblioteca open-source de machine learning;\n",
    "* [src](../src/): pacote com todos os c√≥digos de todas as fun√ß√µes utilit√°rias criadas para esse sistema. Localizado dentro do diret√≥rio `../src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a4f5ea5-7a61-4b66-9a59-c6b7ab71e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.abspath( # Obtendo a vers√£o absoluta normalizada do path ra√≠z do projeto\n",
    "    os.path.join( # Concatenando os paths\n",
    "        os.getcwd(), # Obtendo o path do diret√≥rio dos notebooks\n",
    "        os.pardir # Obtendo a string constante usada pelo OS para fazer refer√™ncia ao diret√≥rio pai\n",
    "    )\n",
    ")\n",
    "# Adicionando o path √† lista de strings que especifica o path de pesquisa para os m√≥dulos\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.transformers_finetuning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acb6b9-6ba5-49e1-a00d-b90a63beeb67",
   "metadata": {},
   "source": [
    "> **Nota**: os c√≥digos para as fun√ß√µes utilit√°rias utilizadas nesse sistema est√£o no script `transformers_finetuning.py` dentro do diret√≥rio `../src/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead2267-48de-4889-aac5-dea58282b080",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Carregando os Dados\n",
    "Vamos ler cada um dos subsets dentro de seus respectivos diret√≥rios dentro do diret√≥rio `../data/preprocessed/` e plotar cada um deles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e38bf67-f145-47b7-bfa7-5149743df283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do train set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10156\n",
      "})\n",
      "Shape do validation set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3385\n",
      "})\n",
      "Shape do test set: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset.load_from_disk('../data/preprocessed/train_dataset')\n",
    "val_set = Dataset.load_from_disk('../data/preprocessed/validation_dataset')\n",
    "test_set = Dataset.load_from_disk('../data/preprocessed/test_dataset')\n",
    "print(f'Shape do train set: {train_set}\\nShape do validation set: {val_set}\\nShape do test set: {test_set}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37646c-d5a8-4829-a7d7-76080f57e6d3",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## Transformers\n",
    "<img align='center' src='../figures/transformers.png' style='width:400px;'>\n",
    "Transformer √© um modelo puramente baseado em attention que foi desenvolvido pelo Google para solucionar alguns problemas com RNNs, como o de ser dif√≠cil de explorar totalmente as vantagens da computa√ß√£o paralela, devido aos seus problemas decorrentes de sua estrutura sequencial. Em um RNN seq2seq, precisamos passar por cada palavra de nosso input, de forma sequencial pelo encoder, e √© feito de uma forma sequencial similar pelo decoder, sem computa√ß√£o paralela. Por esse motivo, n√£o h√° muito espa√ßo para c√°lculos paralelos. Quanto mais palavras temos na sequ√™ncia de input, mais tempo ser√° necess√°rio para process√°-la.\n",
    "\n",
    "Com sequ√™ncias grandes, ou seja, com muitos $T$ steps sequenciais, as informa√ß√µes tendem a se perder na rede (loss of information) e problemas de vanishing gradients surgem relacionados ao comprimento de nossas input sequences. LSTMs e GRUs ajudam um pouco com esses problemas, mas mesmo essas arquiteturas param de funcionar bem quando tentam processar sequ√™ncias muito longas devido ao `information bottleneck`.\n",
    "* `Loss of information`: √© mais dif√≠cil saber se o sujeito √© singular ou plural √† medida que nos afastamos do sujeito.\n",
    "* `Vanishing gradients`: quando calculamos o backprop, os gradients podem se tornar muito pequenos e, como resultado, o modelo n√£o aprender√° nada.\n",
    "\n",
    "Os transformers se baseiam em attention e n√£o exigem nenhum c√°lculo sequencial por layer, sendo necess√°rio apenas um √∫nico step. Al√©m disso, os steps de gradient que precisam ser realizados do √∫ltimo output para o primeiro output em um transformers s√£o apenas 1. Para RNNs, o n√∫mero de steps aumenta com sequ√™ncias mais longas. Por fim, os transformers n√£o sofrem com problemas de vanishing gradients relacionados ao comprimento das sequ√™ncias.\n",
    "\n",
    "Transformers n√£o usam RNNs, attention √© tudo o que precisamos, e apenas algumas transforma√ß√µes lineares e n√£o lineares geralmente s√£o inclu√≠das. O modelo transformers foi introduzido em 2017 por pesquisadores do Google, e desde ent√£o, a arquitetura do transformer se tornou padr√£o para LLMs. Os transformers revolucionaram o campo de NLP.\n",
    "\n",
    "O modelo transformers usa `scaled dot-product attention`. A primeira forma da attention √© muito eficiente em termos de computa√ß√£o e mem√≥ria, porque consiste apenas em opera√ß√µes de multiplica√ß√µes de matrizes. Esse mecanismo √© o n√∫cleo do modelo e permite que o transformers cres√ßa e se torne mais complexo, sendo mais r√°pido e usando menos mem√≥ria do que outras arquiteturas de modelos compar√°veis.\n",
    "\n",
    "No modelo transformers, usaremos a `Multi-Head Attention layer`, essa layer √© executada em paralelo e tem v√°rios mecanismos scaled dot-product attention $h$ e v√°rias transforma√ß√µes lineares dos input queries $Q$, keys $K$ e values $V$. Nessa layer, as transforma√ß√µes lineares s√£o par√¢metros trein√°veis.\n",
    "$$\\text{ Attention}(Q, K, V) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V$$\n",
    "<img align='center' src='../figures/attention.png' style='width:600px;'>\n",
    "\n",
    "Os transformers `encoders` come√ßam com um m√≥dulo multi-head attention que executa a `self-attention` na input sequence. Isso √© seguido por uma residual connection e normaliza√ß√£o, em seguida, uma feed forward layer e outra residual connection e normaliza√ß√£o. A encoder layer √© repetida $N_x$ vezes.\n",
    "* Self-attention: cada palavra no input corresponde a cada outra palavra no input sequence.\n",
    "* Gra√ßas √† self-attention layer, o encoder fornecer√° uma representa√ß√£o contextual de cada um de nossos inputs.\n",
    "\n",
    "\n",
    "O `decoder` √© constru√≠do de forma similar ao encoder, com m√≥dulos multi-head attention, residual connections e normaliza√ß√£o. O primeiro m√≥dulo de attention √© mascarado (`Masked Self-Attention`) de forma que cada posi√ß√£o atenda apenas √†s posi√ß√µes anteriores, ele bloqueia o fluxo de informa√ß√µes para a esquerda. O segundo m√≥dulo de attention (`Encoder-Decoder Attention`) pega o output do encoder e permite que o decoder atenda a todos os itens. Toda essa decoder layer tamb√©m √© repetida v√°rias $N_x$ vezes, uma ap√≥s a outra.\n",
    "$$\\text{ Masked Self-Attention } = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + M \\right) = \\mathrm{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} + \\text{ mask matrix } \\begin{pmatrix} 0 & -\\infty & -\\infty \\\\ 0 & 0 & -\\infty \\\\ 0 & 0 & 0 \\end{pmatrix} \\right)$$\n",
    "\n",
    "Os transformers tamb√©m incorporam um `positional encoding stage` ($PE$), que codifica a posi√ß√£o de cada input na sequ√™ncia, ou seja, as informa√ß√µes sequenciais. Isso √© necess√°rio porque os transformers n√£o usam RNNs, mas a ordem das palavras √© relevante para qualquer idioma. A positional encoding √© trein√°vel, assim como as word embeddings.\n",
    "$$\\begin{align*}\n",
    "& \\text{PE}_{(\\text{pos, }2i)} = \\text{sin}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right) \\\\\n",
    "& \\text{PE}_{(\\text{pos, }2i + 1)} = \\text{cos}\\left( \\frac{\\text{pos}}{10000^{\\frac{2i}{d}}} \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Primeiro, √© calculado o embedding sobre o input e as positional encodings s√£o aplicadas. Ent√£o, isso vai para o encoder que consiste em v√°rias layers de m√≥dulos de Multi-Head Attention, em seguida, o decoder recebe a sequ√™ncia de output deslocada um step para a direita e os outputs do encoder. O output do decoder √© transformado em probabilidades de outputs usando uma layer linear com uma ativa√ß√£o softmax. Essa arquitetura √© f√°cil de paralelizar em compara√ß√£o com os modelos RNNs e pode ser treinada com muito mais efici√™ncia em v√°rias GPUs. Ela tamb√©m pode ser escalada para aprender v√°rias tarefas em datasets cada vez maiores. Transformers s√£o uma √≥tima alternativa aos RNNs e ajudam a superar esses problemas em NLP e em muitos campos que processam dados sequenciais.\n",
    "\n",
    "Faremos o fine-tuning no modelo DistilBERT, que √© um modelo Transformer pequeno, r√°pido, barato e leve, treinado pela destila√ß√£o do modelo base BERT. Ele tem 40% menos par√¢metros que o bert-base-uncased, roda 60% mais r√°pido e preserva mais de 95% do desempenho do BERT conforme medido no benchmark GLUE (General Language Understanding Evaluation).\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) (ü§ó) √© o melhor recurso para transformers pr√©-treinados. Suas bibliotecas de c√≥digo aberto simplificam o download, o fine-tuning e o uso de modelos de transformers como DeepSeek, BERT, Llama, T5, Qwen, GPT-2 e muito mais. E a melhor parte, podemos us√°-los junto com TensorFlow, PyTorch ou Flax. Neste notebook, utilizo transformers ü§ó para usar o modelo `DistilBERT` para classifica√ß√£o de sentimento. Para a etapa do pr√©-processamento, usamos o tokenizador (no notebook `03_preprocessing.ipynb`), e no fine-tuning do checkpoint do DistilBERT `distilbert-base-uncased-finetuned-sst-2-english` pr√©-treinado no c√≥digo abaixo. Para isso inicializamos a classe DistilBertForSequenceClassification e definidos o modelo pr√©-treinado desejado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a315b-c133-4fac-a7f8-f175550ad4e2",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## Transfer Learning\n",
    "Para o modelo transformers, utilizamos a t√©cnica que transfer learning. Ela √© uma das ideias mais poderosas do deep learning, que √†s vezes, podemos pegar o conhecimento que a rede neural aprendeu em uma tarefa e aplicar esse conhecimento em uma tarefa separada. H√° 3 vantagens principais no transfer learning s√£o:\n",
    "* Reduz o tempo de treinamento.\n",
    "* Melhora as previs√µes.\n",
    "* Nos permite usar datasets menores.\n",
    "\n",
    "Se estamos criando um modelo, em vez de treinarmos os pesos do 0, a partir de uma inicializa√ß√£o aleat√≥ria, geralmente progredimos muito mais r√°pido baixando pesos que outra pessoa j√° treinou por dias/semanas/meses na arquitetura da rede neural, as usamos como pr√©-treinamento, e as transferimos para uma nova tarefa semelhante na qual possamos estar interessados. Isso significa que muitas vezes podemos baixar pesos de algoritmos open-source, que outras pessoas levaram semanas ou meses para descobrir, e ent√£o, usamos isso como uma inicializa√ß√£o muito boa para nossa rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9148a-39fe-4150-bc54-7976a67f4e45",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### Fine-tuning\n",
    "Com o modelo pr√©-treinado, aplicamos o fine-tuning. Ent√£o, pegamos os pesos de um modelo pr√©-treinado existente, utilizando transfer learning e, em seguida, ajustamos um pouco para garantir que funcionem na tarefa espec√≠fica em que estamos trabalhando. Digamos que pr√©-treinamos um modelos que prev√™ a avalia√ß√£o de filmes, e agora vamos criar um modelo para avaliar cursos. Uma maneira de fazer isso √©, bloqueando todos os pesos que j√° temos pr√©-treinados e, em seguida, adicionamos uma nova output layer, ou talvez, uma nova feed forward layer e uma output layer que ser√£o treinadas, enquanto mantemos o restante bloqueado e, em seguida, treinamos apenas a nossa nova rede, as novas layers que acabamos de adicionar. Podemos descongelar lentamente as layers, uma de cada vez.\n",
    "\n",
    "Muitas das features de baixo n√≠vel que o modelo pr√©-treinado aprendeu a partir de um corpus muito grande, como a estrutura do texto, a natureza do texto, isso pode ajudar nosso algoritmo a se sair melhor na tarefa de classifica√ß√£o de sentimentos e mais r√°pido ou com menos dados, porque talvez o modelo tenha aprendido o suficiente como s√£o as estruturas de textos diferentes e parte desse conhecimento ser√° √∫til. Ap√≥s excluirmos a output layer de um modelo pr√©-treinado, n√£o precisamos necessariamente criar apenas a output layer, mas podemos criar v√°rias novas layers.\n",
    "\n",
    "Precisamos remover a output layer do modelo pr√©-treinado e adicionar a nossa, porque a rede neural pode ter uma softmax output layer que gera um dos 1000 labels poss√≠veis. Ent√£o removemos essa output layer e criamos a nossa pr√≥pria output layer, nesse caso uma ativa√ß√£o sigmoid.\n",
    "\n",
    "* Com um training set pequeno, pensamos no restante das layers como `congeladas`, ent√£o congelamos os par√¢metros dessas layers, e treinamos apenas os par√¢metros associados √† nossa output layer. Dessa forma obteremos um desempenho muito bom, mesmo com um training set pequeno.\n",
    "\n",
    "* Com um training set maior, n√≥s podemos congelar menos layers e ent√£o treinar as layers que n√£o foram congeladas e a nossa nova output layer. Podemos usar as layers que n√£o est√£o congeladas como inicializa√ß√£o e usar o gradient descent a partir delas, ou tamb√©m podemos eliminar essas layers que n√£o est√£o congeladas e usamos nossas pr√≥prias hidden layers novas e nossa pr√≥pria output layer. Qualquer um desses m√©todos pode valer a pena tentar.\n",
    "\n",
    "* Com um training set muito maior usamos essa rede neural pr√©-treinada e os seus pesos como inicializa√ß√£o e treinamos toda a rede neural, apenas alterando a output layer, com labels que nos importamos.\n",
    "\n",
    "Definindo o checkpoint do modelo pr√©-treinado que faremos o fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b6dd67-2714-4583-81d0-f8db9e01fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e5df1-9a50-43d1-b08b-da5943921f30",
   "metadata": {},
   "source": [
    "Definindo os hiperpar√¢metros com o objeto `TrainingArguments`, usando o objeto `Trainer` do Hugging Face para realizar o fine-tuning do modelo.\n",
    "* O modelo pr√©-treinado com o fine-tuning j√° est√° sendo salvo no diret√≥rio `../models/`, definido na defini√ß√£o dos hiperpar√¢metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1585ffdb-779b-420d-81f3-65fbe73251af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2540' max='2540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2540/2540 7:12:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.592717</td>\n",
       "      <td>0.684184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.697679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.585440</td>\n",
       "      <td>0.712016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.488176</td>\n",
       "      <td>0.756529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.502221</td>\n",
       "      <td>0.761378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.474515</td>\n",
       "      <td>0.775508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.474097</td>\n",
       "      <td>0.764560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.448737</td>\n",
       "      <td>0.778758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.487454</td>\n",
       "      <td>0.767549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.445698</td>\n",
       "      <td>0.818367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.394227</td>\n",
       "      <td>0.821744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.485700</td>\n",
       "      <td>0.402096</td>\n",
       "      <td>0.823441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.470619</td>\n",
       "      <td>0.817294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.383477</td>\n",
       "      <td>0.831915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.392066</td>\n",
       "      <td>0.833519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.384125</td>\n",
       "      <td>0.846788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.389733</td>\n",
       "      <td>0.843293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.395148</td>\n",
       "      <td>0.846830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.388265</td>\n",
       "      <td>0.837942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.351318</td>\n",
       "      <td>0.849480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.309669</td>\n",
       "      <td>0.867014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.387884</td>\n",
       "      <td>0.869130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.325386</td>\n",
       "      <td>0.870885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.344718</td>\n",
       "      <td>0.875784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2540, training_loss=0.4370420598608302, metrics={'train_runtime': 25970.5016, 'train_samples_per_second': 0.782, 'train_steps_per_second': 0.098, 'total_flos': 2690677801500672.0, 'train_loss': 0.4370420598608302, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hiperpar√¢metros do fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/transformers_results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=20,\n",
    "    weight_decay=1e-1,\n",
    "    eval_strategy='steps',\n",
    "    lr_scheduler_type='reduce_lr_on_plateau',\n",
    "    logging_steps=100\n",
    ")\n",
    "# Objeto Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=f1_metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff61dd2-b950-461e-b805-d43abf11a9ca",
   "metadata": {},
   "source": [
    "Avaliando o desempenho do modelo com fine-tuning no train e validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38acb7b3-8745-410e-8db4-7f05352bb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalia√ß√£o do train set: 0.9178\n",
      "Avalia√ß√£o do validation set: 0.8523\n"
     ]
    }
   ],
   "source": [
    "print(f'Avalia√ß√£o do train set: {trainer.evaluate(train_set)[\"eval_f1_score\"]:.4f}\\nAvalia√ß√£o do validation set: {trainer.evaluate(val_set)[\"eval_f1_score\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bfb82-0c3e-430d-b4e4-5debe24f6ca5",
   "metadata": {},
   "source": [
    "Avaliando o desempenho do modelo final com fine-tuning no test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14f3b133-8ef9-46c0-9a67-517822821220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalia√ß√£o do test set: 0.8518\n"
     ]
    }
   ],
   "source": [
    "print(f'Avalia√ß√£o do test set: {trainer.evaluate(test_set)[\"eval_f1_score\"]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
